---
title: "Not enough power for sensitivity: 14C Power and Sensitivity (weeks 4-5?)"
author: "Pete"
date: "2023-07-14"
categories: [14C Power/Sensitivity, Research Design, Project Log]
execute:
  message: false
format: 
  html: 
    code-fold: true
    code-overflow: scroll
---

```{r include=FALSE}
library(tidyverse)
library(broom)
```


Hello everyone! Last time we left this project, we were on the verge of doing some exploratory modelling to start getting a more precise assessment of patterns we spotted during preliminary visualizations. To recap, we are trying to evaluate how much systematic offsets affect the precision of calibrated radiocarbon date ranges through simulating 10k of those models, with known offsets and checking how they vary. So far we spotted that model accuracy changes through time, as well as based on model precision (more accurate ones tend to get the answers wrong more often). We left off with two ways of approaching the exploratory modelling.

-   One approach would treat uncertainty about the mean of the calibration curve as one of the model variables.
-   The other approach would break up the data set based on target dates of the simulated radiocarbon dates, and build logistic models for each time period.

This installment focuses on the outcomes of the second approach. TLDR: if we want to push this any further, we need a lot more simulations than the 10k we've got.

## The models explored

Before going for our main goal, which is estimating how much offset is dangerous for these kinds of measurements during different time periods, we need to have a good model for describing how difference modelled date ranges relate to different combinations of underpinning factors.

In the current approach, I wanted to build seperate models for different points in time, which left me with two variables to consider: - Magnitude of systematic offset (ranging from -50 to 50 radiocarbon years - which are in reality a unit not of time, but of radiocarbon concentration) - Measurement precision (Between 8 and 32 radiocarbon years - less means more precise).

With these two variables four models were possible: 1. Offset only, ignoring any information from measurement precision 2. Precision only, ignoring offset magnitude 3. Both offset and precision 4. Offset, precision, and their interaction.

While I was confident *a priori* that the second model was wrong (and I think you'll agree with me here), it was unclear how models 3 and 4 would compare, or for that matter whether model 1 might not be better on the grounds of parsimony (yes, measurement precision did seem to matter, but maybe not enough to justify it in model inclusion).

I did spend some time two or three years ago chasing my own tail on that one time and time again. This time I decided to go for a simpler solution: I just applied all four models across the board, retaining the "dubious" model 2, as a baseline of bad model choice.

## How the modelling got done

This plan involved a lot of models. To be specific four general model designs times however many bins times two to account for positive and negative offsets (after all, we cannot know if their impact on the data is symmetric in all cases and hence, I was reluctant to just use absolute values).

I did the guts of the modelling with the [purr and broom method from R4DS](https://r4ds.had.co.nz/many-models.html): compressed the binned data using nest() and then applied the purpose built model functions to the relevant data. Hence the whole modelling process was over in a matter of maybe a half-hour (at least after I've cleared off the cobwebs on how the thing worked in the first place :stuck_out_tongue:).

```{r, output = FALSE}


### Set-up for fifty bins
singles_data_url <- "https://raw.githubusercontent.com/pete-jacobsson/14C-power-sensitivity/main/simulation_results/singles_011_results.csv" 
single_cals <- read_csv(url(singles_data_url))

###Seperate pos from neg offsets and group things by cal curve 
single_cals_modelled <- single_cals %>%
  mutate (
    offset_pos = if_else(offset_magnitude > 0, TRUE, FALSE),
    offset_magnitude = if_else(offset_pos, 
                               offset_magnitude, offset_magnitude * -1),
    # Changing negs on offset magnitude to have a consistent direction at downstream visualization.
    binned_targets = ntile(target_year, 50)
  ) %>%
  select(-target_year)

### ntile() wouldn't return the relevant target years, so it needs to be done as its own thing.
single_cals_modelled <- single_cals %>%
  mutate(binned_targets = ntile(target_year, 50)) %>%
  group_by(binned_targets) %>%
  summarize( #Easy way to get years for the indiv bins, without too muchj headach
    target_year = min(target_year)
    ) %>%
  inner_join(single_cals_modelled) %>% #Join the binned DF back in. No, I don't like this either!
  group_by(offset_pos, target_year) %>%
  nest()
```

```{r, output = FALSE}
### Build the model functions
offset_only_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ offset_magnitude, data = singles_data, 
      family = binomial)
}

sigma_only_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error, data = singles_data, 
      family = binomial)
}

offset_sigma_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude, data = singles_data, 
      family = binomial)
}

offset_sigma_interact_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + 
        measurement_error * offset_magnitude, 
      data = singles_data, family = binomial)
}


offset_only_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ offset_magnitude, data = singles_data, 
      family = binomial)
}

sigma_only_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error, data = singles_data, 
      family = binomial)
}

offset_sigma_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude, data = singles_data, 
      family = binomial)
}

offset_sigma_interact_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + 
        measurement_error * offset_magnitude, 
      data = singles_data, family = binomial)
}

## Note to future self: some smart pivoting earlier on could have simplified this by half :)

```

```{r, output = FALSE}
### Model execution
single_cals_modelled <- single_cals_modelled %>%
  mutate(
    offset_only_acc68 = map(data, offset_only_acc68),
    sigma_only_acc68 = map(data, sigma_only_acc68),
    offset_sigma_acc68 = map(data, offset_sigma_acc68),
    offset_sigma_interact_acc68 = map(data, offset_sigma_interact_acc68),
    offset_only_acc95 = map(data, offset_only_acc95),
    sigma_only_acc95 = map(data, sigma_only_acc95),
    offset_sigma_acc95 = map(data, offset_sigma_acc95),
    offset_sigma_interact_acc95 = map(data, offset_sigma_interact_acc95)
  )
```

```{r}
### Now get the numbers out of the models - aka the actual hard bit XP

model_names <- colnames(single_cals_modelled)[4:11] ##These will tell us what results go with what model

single_cals_log_results <- data.frame()
single_cals_log_diagnostics <- data.frame()

for (model in model_names) {
  ## This will get a little experimental - we are trying to create a big old table!
  ## First, we thin down the DF to the model of interest
  temp_results <- single_cals_modelled %>%
    select(1, 2, all_of(model)) %>% #All of used to address deprecation
    rename(glm_list = 3) %>% #This rename allows map to work correctly
    mutate(
      logistic_results = map(glm_list, tidy), # Do the map and also take note of which model
      model = model
    ) %>%
    select(-glm_list) %>% # Gets rid of the actual models (necessary for saving as csv)
    unnest(logistic_results)# Unnests results
  
  temp_diagnostics <- single_cals_modelled %>% ## Similar to above
    select(1, 2, all_of(model)) %>%
    rename(glm_list = 3) %>%
    mutate(
      logistic_diagnostics = map(glm_list, glance),
      model = model
    ) %>%
    select(-glm_list) %>%
    unnest(logistic_diagnostics)
  
  single_cals_log_results <- rbind(single_cals_log_results, temp_results)
  single_cals_log_diagnostics <- rbind(single_cals_log_diagnostics, 
                                       temp_diagnostics)

}
```



## Model fit quality

The first thing I did after getting the models run was to have a look at the quality parameters, the AIC and BIC estimates. If I were lucky, these would help me choose the best model for the job fast. As always, the reality was not as pleasant however, but at least we got one interesting observation.

```{r}
single_cals_log_diagnostics %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "68")) %>%
  ggplot(aes(x = target_year, y = AIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos) +
  theme_bw() +
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  labs(
    subtitle = "68.2% HPD Area models: AIC values",
    x = "Cal yrs BP"
    )

```

When I too a look at the [AIC values](https://www.r-bloggers.com/2018/04/how-do-i-interpret-the-aic/) for the models built for different points in time, the first thing I noticed was the sigma_only model doing worse of all our models across the board (with AIC less is better). This was very, very **reassuring** - at least one expectation got fulfilled. What was a bit more challenging was that there was no clear winner model.


```{r}
single_cals_log_diagnostics %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "95")) %>%
  ggplot(aes(x = target_year, y = AIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos) +
  theme_bw() +
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  labs(
    subtitle = "95.4% HPD Area models: AIC values",
    x = "Cal yrs BP"
    )

```

Looking at the 95.4% HPD ranges we have a similar story, but with a few twists. First of all, the AICs are lower than for the 68.2% HPD ranges. This makes sense: in the 68.2% case, we expect a lot more determinations to be off-target because of stochastic nature of measurement. This inbtroduces way more noise, and thus the models have a harder time estimating. In the 95.4% case, the effects of other variables should be more decisive.

But taking one look at the graphs, it is clear that the real interesting bit is elsewhere. Model quality is time-dependent - the further back you go, the better the models are at explaining variation in the data. As the big difference in calibration curve structure is that of curve precision, we can expect that to have some impact on model explanatory power - perhaps by providing just a little less precise calibrations, thus further reducing stochastic effects of measurement uncertainty.


## Model results discussion

While the model quality parameters provided some interesting insights, they did not bring us closer to our goal of choosing a model that would be good for predicting whether radiocarbon determinations subject to certain magnitudes of systematic offset will produce accurate calibrated date ranges.


Base it off the research notes

Twist - we lost the time dimension

## Try again: bigger bins

## Conclusion

Not enough power for precision. Important bit - What changes in this characterization might be the baseline (intercept). This makes sense.
