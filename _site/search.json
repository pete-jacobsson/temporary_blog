[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Temporary Blog",
    "section": "",
    "text": "Choosing models for singles: 14C Power and Sensitivity (weeks 2-3)\n\n\n\n\n\n\n\n14C Power/Sensitivity\n\n\nResearch Design\n\n\nProject Log\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nPete\n\n\n\n\n\n\n\n\nTo resurrect a project: 14C Power and Sensitivity (week 1)\n\n\n\n\n\n\n\n14C Power/Sensitivity\n\n\nResearch Design\n\n\nProject Log\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nPete\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hiya, I’m Pete and welcome to my temporary blog. It is temporary, because I know that the format will change a few times before I settle on something I like. It is also called a temporary blog, while I try to figure out its identity and come up with a more meaningful name.\nWhat does the temporary blog cover? As above, of this I am not sure, but the material in the pipeline consists of project journals, covering various pieces of personal data projects, as well as essays on topics related to research design to speculations on how LLMs may shift the balance between humanities and quantitative sciences in the modern world.\nDrop by for regular updates!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/test_post_1/index.html",
    "href": "posts/test_post_1/index.html",
    "title": "Test Post #1",
    "section": "",
    "text": "This is the first test post. It only contains text. Lets see if it shows up when I render the blog.\nNow with some headers:"
  },
  {
    "objectID": "posts/test_post_1/index.html#a-subheader-of-lorem-i.",
    "href": "posts/test_post_1/index.html#a-subheader-of-lorem-i.",
    "title": "Test Post #1",
    "section": "A subheader of Lorem I.",
    "text": "A subheader of Lorem I.\nAliquam venenatis sollicitudin libero sit amet semper. Duis eleifend, lorem quis vehicula feugiat, libero ex vestibulum ligula, id pellentesque dolor metus in neque. Mauris ullamcorper, lorem quis mollis convallis, libero nisi luctus nisl, viverra dapibus lorem eros in quam. Pellentesque non nibh convallis, volutpat ex at, fringilla ante. Vivamus quis nulla dui. Phasellus ut nulla vel sapien efficitur scelerisque. Vivamus consectetur quam at justo finibus posuere. Etiam ac magna laoreet augue finibus viverra. Aenean molestie massa vel nisi efficitur semper. Nullam sodales nisi urna, non malesuada elit ultricies id.\n\nA sub-sub-header of Lorem I.\nAliquam venenatis sollicitudin libero sit amet semper. Duis eleifend, lorem quis vehicula feugiat, libero ex vestibulum ligula, id pellentesque dolor metus in neque. Mauris ullamcorper, lorem quis mollis convallis, libero nisi luctus nisl, viverra dapibus lorem eros in quam. Pellentesque non nibh convallis, volutpat ex at, fringilla ante. Vivamus quis nulla dui. Phasellus ut nulla vel sapien efficitur scelerisque. Vivamus consectetur quam at justo finibus posuere. Etiam ac magna laoreet augue finibus viverra. Aenean molestie massa vel nisi efficitur semper. Nullam sodales nisi urna, non malesuada elit ultricies id."
  },
  {
    "objectID": "posts/post_with_two_pics/index.html",
    "href": "posts/post_with_two_pics/index.html",
    "title": "Test Post #1",
    "section": "",
    "text": "Another test post. Here I put in some code:\n\ntwo <- 1 + 1\n\nDoes new text even show up? As well as some in-line code\n\n\n\nAnother Random Picture"
  },
  {
    "objectID": "test_posts/test_post_1/index.html",
    "href": "test_posts/test_post_1/index.html",
    "title": "Test Post #1",
    "section": "",
    "text": "This is the first test post. It only contains text. Lets see if it shows up when I render the blog.\nNow with some headers:"
  },
  {
    "objectID": "test_posts/test_post_1/index.html#a-subheader-of-lorem-i.",
    "href": "test_posts/test_post_1/index.html#a-subheader-of-lorem-i.",
    "title": "Test Post #1",
    "section": "A subheader of Lorem I.",
    "text": "A subheader of Lorem I.\nAliquam venenatis sollicitudin libero sit amet semper. Duis eleifend, lorem quis vehicula feugiat, libero ex vestibulum ligula, id pellentesque dolor metus in neque. Mauris ullamcorper, lorem quis mollis convallis, libero nisi luctus nisl, viverra dapibus lorem eros in quam. Pellentesque non nibh convallis, volutpat ex at, fringilla ante. Vivamus quis nulla dui. Phasellus ut nulla vel sapien efficitur scelerisque. Vivamus consectetur quam at justo finibus posuere. Etiam ac magna laoreet augue finibus viverra. Aenean molestie massa vel nisi efficitur semper. Nullam sodales nisi urna, non malesuada elit ultricies id.\n\nA sub-sub-header of Lorem I.\nAliquam venenatis sollicitudin libero sit amet semper. Duis eleifend, lorem quis vehicula feugiat, libero ex vestibulum ligula, id pellentesque dolor metus in neque. Mauris ullamcorper, lorem quis mollis convallis, libero nisi luctus nisl, viverra dapibus lorem eros in quam. Pellentesque non nibh convallis, volutpat ex at, fringilla ante. Vivamus quis nulla dui. Phasellus ut nulla vel sapien efficitur scelerisque. Vivamus consectetur quam at justo finibus posuere. Etiam ac magna laoreet augue finibus viverra. Aenean molestie massa vel nisi efficitur semper. Nullam sodales nisi urna, non malesuada elit ultricies id."
  },
  {
    "objectID": "test_posts/post_with_two_pics/index.html",
    "href": "test_posts/post_with_two_pics/index.html",
    "title": "Test Post #1",
    "section": "",
    "text": "Another test post. Here I put in some code:\n\ntwo <- 1 + 1\n\nDoes new text even show up? As well as some in-line code\n\n\n\nAnother Random Picture"
  },
  {
    "objectID": "test_posts/post-with-code/index.html",
    "href": "test_posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "test_posts/welcome/index.html",
    "href": "test_posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/power_sensitivity_entry_1/index.html",
    "href": "posts/power_sensitivity_entry_1/index.html",
    "title": "To resurrect a project: 14C Power and Sensitivity (week 1)",
    "section": "",
    "text": "Hello everyone and welcome to what is likely first post on my blog. Today, I’ll provide a b it of a background on a project I am about to resurrect and which some of you might find interesting. Since far too long ago I was working on and off on developing a better idea of how to do quantitative research design in radiocarbon dating, in other words, how to stick a probability distribution on an estimate of how precise the results we’ll be before we take any measurements (power analysis) and evaluate whether the end models are as trustworthy as the HPD areas imply (sensitivity analysis).\nThe Readme in the Github project repository should contain all the details in serious academic format. However, for those not familiar with radiocarbon chronological modelling here are the bare bone basics:\n\nRadiocarbon dating is based on estimating how much of the radioactive 14C isotope has decayed from an organic sample. This involves measuring atoms that show at very small rates, with all the associated contamination risks and precision difficulties.\nRadiocarbon levels fluctuated in the past, so measurement need calibrated to get a meaningful date. How do we do this? We use a calibration curve based on 14C measurements of stuff for which we know the age through some other means (dendrochronology is best, but for the older periods other isotopic dating methods, or varve counting are often used).\nIn most serious radiocarbon work we no longer use single dates - rather we take series of measurements and estimate dates of events of interest (e.g. when was this site abandoned) by using Bayesian chronological modelling.\nUnless you’re French speaking, you’re likely doing your Bayesian chronological modelling using OxCal.\nOxCal is great at what it does, but it does not provide functionality to easily simulate a lot of models with slight changes to simulation input parameters. Which is a convoluted way of saying: if you want to do enough simulations to do quantitative research design in OxCal alone, you will spend a lot of time on manual drudgery.\n\nThe project that I’m off to resurrect was about cutting out the manual part - instead of passing new simulations into OxCal and taking notes on how individual results looked, I wrote scripts to generate simulations from tables of inputs, scripts to pass the input models to OxCal via the command line, and scripts to parse the results out of the OxCal output .js files (no, it’s not a typo - OxCal results come in .js). I got as far as having simulated 300k+ different models and setting up a Shiny App to visualize the outcomes when the pandemic fight call-up came along and I no longer had time to work on this. Until now. Over the next few weeks I hope to revisit the project and at least complete the analytics on the simulations produced to date (I have ambitions to package the functions to simulate and read the .js files… but I haven’t opened to code yet).\nSee you next week with (hopefully) some initial results on the sensitivity side!"
  },
  {
    "objectID": "posts/power_sensitivity_entry_2/index.html",
    "href": "posts/power_sensitivity_entry_2/index.html",
    "title": "Choosing models for singles: 14C Power and Sensitivity (weeks 2-3)",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nWelcome back everyone! This took a little longer to put together than anticipated. Truth is, as usual with data projects, there was always the nagging little voice arguing that we should follow the data a little longer… so what’s up on the project?\nThe plan for week 2 was to have a look at the old project archive and start figuring out what can be done. As discussed in the previous post, the project itself is all about running thousands of simulated radiocarbon dates and modelling them to then be able to suggest data-driven decisions for actual sampling. Now, the simulations all got done (or so I thought!) before I went off to fight the pandemic two years ago, alongside some tools for data exploration. So I some time over the past couple of weeks digging through those old project folders, choosing a particular set of simulations to focus on and taking things forward from there. I’ll write up the first batch of analytics next week, this week we’ll focus on the actual stuff that was left behind from the academic days."
  },
  {
    "objectID": "posts/power_sensitivity_entry_2/index.html#things-left-behind",
    "href": "posts/power_sensitivity_entry_2/index.html#things-left-behind",
    "title": "Choosing models for singles: 14C Power and Sensitivity (weeks 2-3)",
    "section": "Things left behind",
    "text": "Things left behind\nSo the project itself covered three types of radiocarbon models: single calibrated dates, radiocarbon wiggle-match dates, and relatively simple “sequence” models.\n\nThe first type of model is just Bayesian calibration of a 14C determination to a calendar date (see this post on calibration curious what that is).\nThe second type of model is a Bayesian wiggle-match date - a bunch of radiocarbon measurements where we know how much time elapsed between the samples - most often gets used for tree-rings (you can find the model ran in a spreadsheet here if you want the details).\nThe third type of model were “Sequences” - groups of dates where we have some idea of contemporaneity and ordering of the dates, but no information on their exact chronological relationship. So, for example, we know that bone A got deposited before bone B, but no idea how many years apart. Unlike single calibrations and wiggle-match dates, these kinds of models come with multiplicity of parameters and in general use MCMC. If you want to know more, check out the intro on the Historic England website.\n\nFor each of three kinds of models, I’ve run several different kinds of simulations. Some focused on particular time periods, others on how results changed with different calibration curves, while other still were concerned with obtaining large mass of simulated measurements over different time periods. Looking through the project archive I found the last category most interesting, and thought I’d start with it. However, before going right to the bit that I wanted to do for two years now - studying the wiggle-match date models, I though I’d begin with single calibrations to brush up on some key tools (and practice new ones)."
  },
  {
    "objectID": "posts/power_sensitivity_entry_2/index.html#single-calibrations-how-did-they-get-simulated-and-how-many-ive-got",
    "href": "posts/power_sensitivity_entry_2/index.html#single-calibrations-how-did-they-get-simulated-and-how-many-ive-got",
    "title": "Choosing models for singles: 14C Power and Sensitivity (weeks 2-3)",
    "section": "Single calibrations: how did they get simulated and how many I’ve got?",
    "text": "Single calibrations: how did they get simulated and how many I’ve got?\nBefore we move forward you might ask, how did the simulated radiocarbon dates get simulated. The procedure is quite simple - you can find it outlined in code in this file here. In essence, we begin with a know calibration curve, we choose a systematic offset at random. Remember, large part of the project is to check how sensitive we are to such offsets - by the way I set the offset to between -50 and 50 14C year - which is at the limit of difficult to spot lab screw-ups combined with weird atmospheric effects. We then get a vector of measurement errors - the known uncertainty around our simulated measurements. We then choose a point at random one the calibration curve, check the expected 14C age, shift it by the offset, and draw a random number from a normal distribution with a standard deviation corresponding to the measurement error. These can then get passed to the calibration program OxCal via the command line. Once we get the calibrated results we check whether the 68.2% and 95.4% HPD areas estimated by OxCal contain the known target dates.\nLooking through the project archive, this leg included 10,000 simulations from across the tree-ring based part of the calibration curve - looking at the period from zero to 12,300 Before Present (cal BP). The first step was to re-acquaint myself with the data. Thankfully there was a Shiny App that I built two years ago for just that purpose…"
  },
  {
    "objectID": "posts/power_sensitivity_entry_2/index.html#so-what-are-the-interesting-things-about-these-simulations",
    "href": "posts/power_sensitivity_entry_2/index.html#so-what-are-the-interesting-things-about-these-simulations",
    "title": "Choosing models for singles: 14C Power and Sensitivity (weeks 2-3)",
    "section": "So what are the interesting things about these simulations?",
    "text": "So what are the interesting things about these simulations?\nThere are a few things that can be said about the main singles data set, but the things that I found relevant had to do specifically with the accuracy of the simulations under different offsets.\nFirst, as expected for both 95.4% and 68.2% HPD areas we can see that there is some area of grace with regards to systematic offsets - and that measurements with very small offsets can perform better than the calculated HPD area implies.\n\n\nCode\nsingles_data_url <- \"https://raw.githubusercontent.com/pete-jacobsson/14C-power-sensitivity/main/simulation_results/singles_011_results.csv\"   ##Thank you to https://lokraj.me/post/download-github-data/ for the tutorial!\nsingles_data <- read_csv(url(singles_data_url))"
  }
]