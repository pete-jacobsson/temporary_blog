[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Temporary Blog",
    "section": "",
    "text": "What’s in a generic data analyst job description?\n\n\n\n\n\n\n\nData Analytics\n\n\nProject Log\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nPete\n\n\n\n\n\n\n\n\nNot enough power for sensitivity: Compromises (weeks 6-10?)\n\n\n\n\n\n\n\n14C Power/Sensitivity\n\n\nResearch Design\n\n\nProject Log\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\nPete\n\n\n\n\n\n\n\n\nNot enough power for sensitivity: 14C Power and Sensitivity (weeks 4-5?)\n\n\n\n\n\n\n\n14C Power/Sensitivity\n\n\nResearch Design\n\n\nProject Log\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nPete\n\n\n\n\n\n\n\n\nChoosing models for singles: 14C Power and Sensitivity (weeks 2-3)\n\n\n\n\n\n\n\n14C Power/Sensitivity\n\n\nResearch Design\n\n\nProject Log\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2023\n\n\nPete\n\n\n\n\n\n\n\n\nTo resurrect a project: 14C Power and Sensitivity (week 1)\n\n\n\n\n\n\n\n14C Power/Sensitivity\n\n\nResearch Design\n\n\nProject Log\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nPete\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hiya, I’m Pete and welcome to my temporary blog. It is temporary, because I know that the format will change a few times before I settle on something I like. It is also called a temporary blog, while I try to figure out its identity and come up with a more meaningful name.\nWhat does the temporary blog cover? As above, of this I am not sure, but the material in the pipeline consists of project journals, covering various pieces of personal data projects, as well as essays on topics related to research design to speculations on how LLMs may shift the balance between humanities and quantitative sciences in the modern world.\nDrop by for regular updates!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/test_post_1/index.html",
    "href": "posts/test_post_1/index.html",
    "title": "Test Post #1",
    "section": "",
    "text": "This is the first test post. It only contains text. Lets see if it shows up when I render the blog.\nNow with some headers:"
  },
  {
    "objectID": "posts/test_post_1/index.html#a-subheader-of-lorem-i.",
    "href": "posts/test_post_1/index.html#a-subheader-of-lorem-i.",
    "title": "Test Post #1",
    "section": "A subheader of Lorem I.",
    "text": "A subheader of Lorem I.\nAliquam venenatis sollicitudin libero sit amet semper. Duis eleifend, lorem quis vehicula feugiat, libero ex vestibulum ligula, id pellentesque dolor metus in neque. Mauris ullamcorper, lorem quis mollis convallis, libero nisi luctus nisl, viverra dapibus lorem eros in quam. Pellentesque non nibh convallis, volutpat ex at, fringilla ante. Vivamus quis nulla dui. Phasellus ut nulla vel sapien efficitur scelerisque. Vivamus consectetur quam at justo finibus posuere. Etiam ac magna laoreet augue finibus viverra. Aenean molestie massa vel nisi efficitur semper. Nullam sodales nisi urna, non malesuada elit ultricies id.\n\nA sub-sub-header of Lorem I.\nAliquam venenatis sollicitudin libero sit amet semper. Duis eleifend, lorem quis vehicula feugiat, libero ex vestibulum ligula, id pellentesque dolor metus in neque. Mauris ullamcorper, lorem quis mollis convallis, libero nisi luctus nisl, viverra dapibus lorem eros in quam. Pellentesque non nibh convallis, volutpat ex at, fringilla ante. Vivamus quis nulla dui. Phasellus ut nulla vel sapien efficitur scelerisque. Vivamus consectetur quam at justo finibus posuere. Etiam ac magna laoreet augue finibus viverra. Aenean molestie massa vel nisi efficitur semper. Nullam sodales nisi urna, non malesuada elit ultricies id."
  },
  {
    "objectID": "posts/post_with_two_pics/index.html",
    "href": "posts/post_with_two_pics/index.html",
    "title": "Test Post #1",
    "section": "",
    "text": "Another test post. Here I put in some code:\n\ntwo <- 1 + 1\n\nDoes new text even show up? As well as some in-line code\n\n\n\nAnother Random Picture"
  },
  {
    "objectID": "test_posts/test_post_1/index.html",
    "href": "test_posts/test_post_1/index.html",
    "title": "Test Post #1",
    "section": "",
    "text": "This is the first test post. It only contains text. Lets see if it shows up when I render the blog.\nNow with some headers:"
  },
  {
    "objectID": "test_posts/test_post_1/index.html#a-subheader-of-lorem-i.",
    "href": "test_posts/test_post_1/index.html#a-subheader-of-lorem-i.",
    "title": "Test Post #1",
    "section": "A subheader of Lorem I.",
    "text": "A subheader of Lorem I.\nAliquam venenatis sollicitudin libero sit amet semper. Duis eleifend, lorem quis vehicula feugiat, libero ex vestibulum ligula, id pellentesque dolor metus in neque. Mauris ullamcorper, lorem quis mollis convallis, libero nisi luctus nisl, viverra dapibus lorem eros in quam. Pellentesque non nibh convallis, volutpat ex at, fringilla ante. Vivamus quis nulla dui. Phasellus ut nulla vel sapien efficitur scelerisque. Vivamus consectetur quam at justo finibus posuere. Etiam ac magna laoreet augue finibus viverra. Aenean molestie massa vel nisi efficitur semper. Nullam sodales nisi urna, non malesuada elit ultricies id.\n\nA sub-sub-header of Lorem I.\nAliquam venenatis sollicitudin libero sit amet semper. Duis eleifend, lorem quis vehicula feugiat, libero ex vestibulum ligula, id pellentesque dolor metus in neque. Mauris ullamcorper, lorem quis mollis convallis, libero nisi luctus nisl, viverra dapibus lorem eros in quam. Pellentesque non nibh convallis, volutpat ex at, fringilla ante. Vivamus quis nulla dui. Phasellus ut nulla vel sapien efficitur scelerisque. Vivamus consectetur quam at justo finibus posuere. Etiam ac magna laoreet augue finibus viverra. Aenean molestie massa vel nisi efficitur semper. Nullam sodales nisi urna, non malesuada elit ultricies id."
  },
  {
    "objectID": "test_posts/post_with_two_pics/index.html",
    "href": "test_posts/post_with_two_pics/index.html",
    "title": "Test Post #1",
    "section": "",
    "text": "Another test post. Here I put in some code:\n\ntwo <- 1 + 1\n\nDoes new text even show up? As well as some in-line code\n\n\n\nAnother Random Picture"
  },
  {
    "objectID": "test_posts/post-with-code/index.html",
    "href": "test_posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "test_posts/welcome/index.html",
    "href": "test_posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/power_sensitivity_entry_1/index.html",
    "href": "posts/power_sensitivity_entry_1/index.html",
    "title": "To resurrect a project: 14C Power and Sensitivity (week 1)",
    "section": "",
    "text": "Hello everyone and welcome to what is likely first post on my blog. Today, I’ll provide a b it of a background on a project I am about to resurrect and which some of you might find interesting. Since far too long ago I was working on and off on developing a better idea of how to do quantitative research design in radiocarbon dating, in other words, how to stick a probability distribution on an estimate of how precise the results we’ll be before we take any measurements (power analysis) and evaluate whether the end models are as trustworthy as the HPD areas imply (sensitivity analysis).\nThe Readme in the Github project repository should contain all the details in serious academic format. However, for those not familiar with radiocarbon chronological modelling here are the bare bone basics:\n\nRadiocarbon dating is based on estimating how much of the radioactive 14C isotope has decayed from an organic sample. This involves measuring atoms that show at very small rates, with all the associated contamination risks and precision difficulties.\nRadiocarbon levels fluctuated in the past, so measurement need calibrated to get a meaningful date. How do we do this? We use a calibration curve based on 14C measurements of stuff for which we know the age through some other means (dendrochronology is best, but for the older periods other isotopic dating methods, or varve counting are often used).\nIn most serious radiocarbon work we no longer use single dates - rather we take series of measurements and estimate dates of events of interest (e.g. when was this site abandoned) by using Bayesian chronological modelling.\nUnless you’re French speaking, you’re likely doing your Bayesian chronological modelling using OxCal.\nOxCal is great at what it does, but it does not provide functionality to easily simulate a lot of models with slight changes to simulation input parameters. Which is a convoluted way of saying: if you want to do enough simulations to do quantitative research design in OxCal alone, you will spend a lot of time on manual drudgery.\n\nThe project that I’m off to resurrect was about cutting out the manual part - instead of passing new simulations into OxCal and taking notes on how individual results looked, I wrote scripts to generate simulations from tables of inputs, scripts to pass the input models to OxCal via the command line, and scripts to parse the results out of the OxCal output .js files (no, it’s not a typo - OxCal results come in .js). I got as far as having simulated 300k+ different models and setting up a Shiny App to visualize the outcomes when the pandemic fight call-up came along and I no longer had time to work on this. Until now. Over the next few weeks I hope to revisit the project and at least complete the analytics on the simulations produced to date (I have ambitions to package the functions to simulate and read the .js files… but I haven’t opened to code yet).\nSee you next week with (hopefully) some initial results on the sensitivity side!"
  },
  {
    "objectID": "posts/power_sensitivity_entry_2/index.html",
    "href": "posts/power_sensitivity_entry_2/index.html",
    "title": "Choosing models for singles: 14C Power and Sensitivity (weeks 2-3)",
    "section": "",
    "text": "Welcome back everyone! This took a little longer to put together than anticipated. Truth is, I kinda got carried away with the analytics, turned around and thought “oops, time to write up things that happened a couple weeks ago”. So here they are.\nThe plan for this post was to have a look at the old project archive and start figuring out what can be done. As discussed in the previous post, the project itself is all about running thousands of simulated radiocarbon dates and modelling them to make actual sampling more data-driven. The simulations all got done before I went off to fight the pandemic two years ago, alongside some tools for data exploration. So I took some time over the past couple of weeks digging through those old project folders, choosing a particular set of simulations to focus on and taking things forward from there. This week we’ll focus on the actual stuff that was left behind from the academic days, as well as beginning to explore the simulated data for single radiocarbon calibrations."
  },
  {
    "objectID": "posts/power_sensitivity_entry_2/index.html#things-left-behind",
    "href": "posts/power_sensitivity_entry_2/index.html#things-left-behind",
    "title": "Choosing models for singles: 14C Power and Sensitivity (weeks 2-3)",
    "section": "Things left behind",
    "text": "Things left behind\nSo the project itself covered three types of radiocarbon models: single calibrated dates, radiocarbon wiggle-match dates, and relatively simple “sequence” models.\n\nThe first type of model is just Bayesian calibration of a 14C determination to a calendar date (see this post on calibration if you’re curious what that is).\nThe second type of model is a Bayesian wiggle-match date - a bunch of radiocarbon measurements where we know how much time elapsed between the samples - most often gets used for tree-rings (you can find the model ran in a spreadsheet here).\nThe third type of model were “Sequences” - groups of dates where we have some idea of contemporaneity and ordering of the dates, but no information on their exact chronological relationship. So, for example, we know that bone A got deposited before bone B, but no idea how many years apart. Unlike single calibrations and wiggle-match dates, these kinds of models come with multiplicity of parameters and in general use MCMC. If you want to know more, check out the intro on the Historic England website.\n\nFor each of three kinds of models, I’ve run several different kinds of simulations. Some focused on particular time periods, others on how results changed with different calibration curves, while other still were concerned with obtaining large mass of simulated measurements over different time periods. Looking through the project archive I found that last category most interesting, and thought I’d start with it. I though I’d begin with single calibrations to brush up on some key tools, practice new ones, and de-cobweb the brain."
  },
  {
    "objectID": "posts/power_sensitivity_entry_2/index.html#single-calibrations-how-did-they-get-simulated-and-how-many-ive-got",
    "href": "posts/power_sensitivity_entry_2/index.html#single-calibrations-how-did-they-get-simulated-and-how-many-ive-got",
    "title": "Choosing models for singles: 14C Power and Sensitivity (weeks 2-3)",
    "section": "Single calibrations: how did they get simulated and how many I’ve got?",
    "text": "Single calibrations: how did they get simulated and how many I’ve got?\nOver the next few installments we’ll be focusing on whether the calibrated date estimates we simulate are accurate given different offsets and other model parameters. Before we move forward you might ask how did the simulated radiocarbon dates get simulated in the first place? The procedure is quite simple - you can find it outlined in code in this file here. In essence, it’s all about looking at a value on the calibration curve, moving it by a bit to simulate a systematic offset, and then drawing a normally distributed random number to simulate an actual measurement. The curve itself is a marvel of statistics (check it out), but the bit we work with is a simple table of calendar years, alongside their expected values on the radiocarbon scale and estimate uncertainty. Once I’ve had the curve, I chose a systematic offset at random - after all, a large part of the project is to check how sensitive we are to such offsets. I set those offsets to between -50 and 50 14C year, which is at the limit of difficult to spot lab screw-ups combined with weird atmospheric effects. After then it’s just about following the recipe:\n\nGet a vector of measurement errors (this is the known uncertainty around our simulated measurements) .\nChoose a point at random one the calibration curve.\nCheck the expected 14C age at our chosen spot.\nShift the chosen value by the offset.\nDraw a random number from a normal distribution with a standard deviation corresponding to the measurement error.\nPaste the random numbers together and calibrate using the program OxCal (I just passed the files via the command line from R).\n\nOnce we get the calibrated results we check whether the 68.2% and 95.4% HPD areas estimated by OxCal contain the known target dates and calculate a few other things to which we might return in a few weeks.\nLooking through the project archive, this leg included 10,000 simulations from across the tree-ring based part of the calibration curve - looking at the period from zero to 12,300 Before Present (cal BP). The first step was to re-acquaint myself with the data. Thankfully there was a Shiny App that I built two years ago for just that purpose…"
  },
  {
    "objectID": "posts/power_sensitivity_entry_2/index.html#so-what-are-the-interesting-things-about-these-simulations",
    "href": "posts/power_sensitivity_entry_2/index.html#so-what-are-the-interesting-things-about-these-simulations",
    "title": "Choosing models for singles: 14C Power and Sensitivity (weeks 2-3)",
    "section": "So what are the interesting things about these simulations?",
    "text": "So what are the interesting things about these simulations?\nThere are a few things that can be said about the main singles data set, but the things that I found relevant had to do specifically with the accuracy of the simulations under different offsets. A note on notation before going forward:\n\nTarget years are in Calibrated Years BP (Cal BP) - normal calendar time, counted backwards from 1950. Yes, the future is now.\nOffset magnitude and measurement uncertainties are counted in radiocarbon years (14C yrs). These are not actual time units, but rather transformed estimates of radiocarbon concentration. Think of them as standardized units for this type of analysis. Don’t worry, after you get your PhD in radiocarbon, it’ll all seem natural!\n\nOK, lets have a look at the three main things that came out of initial visualization: offset impacts, time impacts, and measurement precision impacts.\n\n\nCode\n##Loading the data - use URL to github - better reproducibility and no duplication.\n\nsingles_data_url <- \"https://raw.githubusercontent.com/pete-jacobsson/14C-power-sensitivity/main/simulation_results/singles_011_results.csv\"   ##Thank you to https://lokraj.me/post/download-github-data/ for the tutorial!\nsingles_data <- read_csv(url(singles_data_url))\n\n\n\nOffset magnitude and accuracy\n\n\nCode\n## Do the transforms necessary and plot the figures\n## Want to have the bars in the range from zero to one, for easy compare with HPD areas\n\nsingles_data %>%\n  mutate(\n    offset_magnitude = round(offset_magnitude)\n    ) %>%\n  group_by(offset_magnitude) %>%\n  summarize(\n    `68.2% HPD Area` = mean(accuracy_68),\n    `95.4% HPD Area` = mean(accuracy_95) ##Naming to work for figures\n  ) %>%\n  pivot_longer(!offset_magnitude, names_to = \"hpd_area\", \n               values_to = \"ratio_accurate\") %>%\n  mutate(line_for_nominal = if_else(str_detect(hpd_area, \"68\"), 0.682, 0.954)) %>%\n  ggplot(aes(x = offset_magnitude, y = ratio_accurate)) +\n  geom_hline(aes(yintercept = line_for_nominal), linetype = \"dashed\") +\n  geom_bar(stat = 'identity', color = \"steelblue\", fill = \"steelblue\") +\n    facet_wrap(~hpd_area) +\n  theme_bw() +\n  labs(\n    x = \"Offset magnitude (14C yrs)\",\n    y = \"Ratio accurate\"\n  ) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.border = element_blank(),\n    strip.background = element_rect(\n     color=\"white\", fill=\"white\"\n     ),\n     text = element_text(family = \"Corbel\")\n    )\n\n\n\n\n\nAs expected for both 95.4% and 68.2% HPD areas we can see that there is some area of grace with regards to systematic offsets - and that measurements with very small offsets can perform better than the calculated HPD area implies. This is important, as it suggests that we can use some kind of a logistic model to estimate how much of a systematic offset the single determinations can take before results become unreliable.\n\n\nAccuracy through time\n\n\nCode\nsingles_data %>%\n  mutate(\n    target_year = plyr::round_any(target_year, 200)\n    ) %>%\n  group_by(target_year) %>%\n  summarize(\n    `68.2% HPD Area` = mean(accuracy_68),\n    `95.4% HPD Area` = mean(accuracy_95) ##Naming to work for figures\n  ) %>%\n  pivot_longer(!target_year, names_to = \"hpd_area\", \n               values_to = \"ratio_accurate\") %>%\n  mutate(line_for_nominal = if_else(str_detect(hpd_area, \"68\"), 0.682, 0.954)) %>%\n  ggplot(aes(x = target_year, y = ratio_accurate)) +\n  geom_hline(aes(yintercept = line_for_nominal), linetype = \"dashed\") +\n  geom_bar(stat = 'identity', color = \"steelblue\", fill = \"steelblue\") +\n    facet_wrap(~hpd_area) +\n  theme_bw() +\n  labs(\n    x = \"Target Year (Cal BP)\",\n    y = \"Ratio accurate\"\n  ) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.border = element_blank(),\n    strip.background = element_rect(\n     color=\"white\", fill=\"white\"\n     ),\n     text = element_text(family = \"Corbel\")\n    )\n\n\n\n\n\nThere might be differences in how offsets affect accuracy based on where we are on the calibration curve; in more recent millennia we tend to get less accurate results. This implies that the sensitivity levels we thought of calculating in the previous section may be different depending on where we are in time.\n\n\nThe more you scatter, the more often you hit\n\n\nCode\n## Do the transforms necessary and plot the figures\n## Want to have the bars in the range from zero to one, for easy compare with HPD areas\n\nsingles_data %>%\n  mutate(\n    measurement_error = round(measurement_error)\n    ) %>%\n  group_by(measurement_error) %>%\n  summarize(\n    `68.2% HPD Area` = mean(accuracy_68),\n    `95.4% HPD Area` = mean(accuracy_95) ##Naming to work for figures\n  ) %>%\n  pivot_longer(!measurement_error, names_to = \"hpd_area\", \n               values_to = \"ratio_accurate\") %>%\n  mutate(line_for_nominal = if_else(str_detect(hpd_area, \"68\"), 0.682, 0.954)) %>%\n  ggplot(aes(x = measurement_error, y = ratio_accurate)) +\n  geom_hline(aes(yintercept = line_for_nominal), linetype = \"dashed\") +\n  geom_bar(stat = 'identity', color = \"steelblue\", fill = \"steelblue\") +\n    facet_wrap(~hpd_area) +\n  theme_bw() +\n  labs(\n    x = \"Measurement error\",\n    y = \"Ratio accurate\"\n  ) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.border = element_blank(),\n    strip.background = element_rect(\n     color=\"white\", fill=\"white\"\n     ),\n     text = element_text(family = \"Corbel\")\n    )\n\n\n\n\n\nIt seems that measurement error has some impact, with greater measurement uncertainties providing more grace. On a conceptual level this makes sense - the less precise the measurement, the greater the chance that the output model will hit the target date by accident. Having said that, I would expect that for measurements with very small systematic offsets, high-precision measurements should do just fine.\n\n\nCode\n### Heatmap here\nsingles_data %>%\n  mutate(\n    target_year = plyr::round_any(target_year, 800),\n    measurement_error = plyr::round_any(measurement_error, 8)\n  ) %>%\n  group_by(target_year, measurement_error, ) %>%\n  summarize(\n    `68.2% HPD Area` = mean(accuracy_68),\n    `95.4% HPD Area` = mean(accuracy_95)\n  ) %>%\n  pivot_longer(c(`68.2% HPD Area`, `95.4% HPD Area`), names_to = \"hpd_area\", \n               values_to = \"ratio_accurate\") %>%\n  ggplot(aes(x = target_year, y = measurement_error)) +\n  geom_raster(aes(fill = ratio_accurate)) +\n  facet_wrap(~hpd_area) +\n  theme_bw() +\n  labs(\n    x = \"Target year\",\n    y = \"Measurement error\"\n  ) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.border = element_blank(),\n    strip.background = element_rect(\n     color=\"white\", fill=\"white\"\n     ),\n     text = element_text(family = \"Corbel\")\n    )\n\n\n`summarise()` has grouped output by 'target_year'. You can override using the\n`.groups` argument.\n\n\n\n\n\nExploring the old Shiny App also highlighted one other interesting interaction. It seems like more precise measurements suffer more in light of systematic offsets in more recent millennia (we can see it by tiles getting darker in the lower-right hand corner of the figures below). This, alongside the other observation about more recent dates being less robust to systematic offsets, suggests that where we are in time may have an impact on the accuracy of single calibrations."
  },
  {
    "objectID": "posts/power_sensitivity_entry_2/index.html#where-next",
    "href": "posts/power_sensitivity_entry_2/index.html#where-next",
    "title": "Choosing models for singles: 14C Power and Sensitivity (weeks 2-3)",
    "section": "Where next?",
    "text": "Where next?\nInitial explorations of the simulation data give us some idea of what to pursue:\n\nWe can have a look at how different offsets affect measurement precision\nWe can explore the temporal dimension\nWe can explore impacts of measurement precision\n\nThe next step, in the next installment will consist of exploring those factors through modelling, with the purpose of bringing us closer to the fundamental question of this leg of the project - how much can the measurements be offset before the results become unreliable? At this stage we know that measurement uncertainty also might need to be taken into account and that something about the calibration curve might have an impact. We also know that the curve is anything but monotonous, with plenty of features attesting to past states of the sun and the Earth’s geomagnetic field and that the precision of the calibration curve becomes less and less the further back we go in time. Hence, our key challenge moving forward will be to figure out just how much those additional variables matter for estimating the range of systematic offsets that radiocarbon determinations can take before the calibrated dates become less accurate than what the HPD area would imply.\nSee you next week! Or two weeks from now, or however this will pan out."
  },
  {
    "objectID": "posts/power_sensitivity_entry_3/index.html",
    "href": "posts/power_sensitivity_entry_3/index.html",
    "title": "Not enough power for sensitivity: 14C Power and Sensitivity (weeks 4-5?)",
    "section": "",
    "text": "Hello everyone! Last time we left this project, we were on the verge of doing some exploratory modelling to start getting a more precise assessment of the patterns we’ve spotted during preliminary visualizations. To recap, we are trying to evaluate how much systematic offsets affect the precision of calibrated radiocarbon date ranges through simulating 10k of those models, with known offsets and checking how they vary. So far we spotted that model accuracy changes through time, as well as based on model precision (more accurate ones tend to get the answers wrong more often). We left off with two ways of approaching the exploratory modelling.\nThis installment focuses on the outcomes of the second approach. TLDR: if I want to follow this approach any further, I’ll need a lot more simulations than the 10k I’ve got. Want to see why? Read on."
  },
  {
    "objectID": "posts/power_sensitivity_entry_3/index.html#the-models-explored",
    "href": "posts/power_sensitivity_entry_3/index.html#the-models-explored",
    "title": "Not enough power for sensitivity: 14C Power and Sensitivity (weeks 4-5?)",
    "section": "The models explored",
    "text": "The models explored\nBefore going for our main goal, which is estimating how much offset is risky for these kinds of measurements during different time periods, we need to have a good model for describing how difference modelled date ranges relate to different combinations of underpinning factors.\nIn the current approach, I wanted to build separate models for different points in time, which left me with two variables to consider:\n\nMagnitude of systematic offset (ranging from -50 to 50 radiocarbon years, these are not a unit of time, but of radiocarbon concentration)\nMeasurement precision (Between 8 and 32 radiocarbon years - less means more precise).\n\nWith these two variables four models were possible:\n\nOffset only, ignoring any information from measurement precision\nPrecision only, ignoring offset magnitude\nBoth offset and precision\nOffset, precision, and their interaction.\n\nWhile I was confident a priori that the second model was wrong (and I think you’ll agree with me here), it was unclear how models 3 and 4 would compare, or for that matter whether model 1 might not be better on the grounds of parsimony (yes, measurement precision did seem to matter, but maybe not enough to justify it in model inclusion).\nI did spend some time two or three years ago chasing my own tail on that one time and time again. This time I decided to go for a simpler solution: I just applied all four models across the board, retaining the “dubious” model 2, as a baseline of bad model choice."
  },
  {
    "objectID": "posts/power_sensitivity_entry_3/index.html#how-the-modelling-was-done",
    "href": "posts/power_sensitivity_entry_3/index.html#how-the-modelling-was-done",
    "title": "Not enough power for sensitivity: 14C Power and Sensitivity (weeks 4-5?)",
    "section": "How the modelling was done",
    "text": "How the modelling was done\nThis plan involved a lot of models. To be specific four general model designs times however many bins times two to account for positive and negative offsets (after all, we cannot know if their impact on the data is symmetric in all cases and hence, I was reluctant to just use absolute values).\nI did the guts of the modelling with the purr and broom method from R4DS: compressed the binned data using nest() and then applied the purpose built model functions to the relevant data. Hence the whole modelling process was over in a matter of maybe a half-hour (at least after I’ve cleared off the cobwebs on how the thing worked in the first place :stuck_out_tongue:).\n\n\nCode\n### Set-up for fifty bins\nsingles_data_url <- \"https://raw.githubusercontent.com/pete-jacobsson/14C-power-sensitivity/main/simulation_results/singles_011_results.csv\" \nsingle_cals <- read_csv(url(singles_data_url))\n\n###Seperate pos from neg offsets and group things by cal curve \nsingle_cals_modelled <- single_cals %>%\n  mutate (\n    offset_pos = if_else(offset_magnitude > 0, TRUE, FALSE),\n    offset_magnitude = if_else(offset_pos, \n                               offset_magnitude, offset_magnitude * -1),\n    # Changing negs on offset magnitude to have a consistent direction at downstream visualization.\n    binned_targets = ntile(target_year, 50)\n  ) %>%\n  select(-target_year)\n\n### ntile() wouldn't return the relevant target years, so it needs to be done as its own thing.\nsingle_cals_modelled <- single_cals %>%\n  mutate(binned_targets = ntile(target_year, 50)) %>%\n  group_by(binned_targets) %>%\n  summarize( #Easy way to get years for the indiv bins, without too muchj headach\n    target_year = min(target_year)\n    ) %>%\n  inner_join(single_cals_modelled) %>% #Join the binned DF back in. No, I don't like this either!\n  group_by(offset_pos, target_year) %>%\n  nest()\n\n\n\n\nCode\n### Build the model functions\noffset_only_acc68 <- function(singles_data) {\n  glm(accuracy_68 ~ offset_magnitude, data = singles_data, \n      family = binomial)\n}\n\nsigma_only_acc68 <- function(singles_data) {\n  glm(accuracy_68 ~ measurement_error, data = singles_data, \n      family = binomial)\n}\n\noffset_sigma_acc68 <- function(singles_data) {\n  glm(accuracy_68 ~ measurement_error + offset_magnitude, data = singles_data, \n      family = binomial)\n}\n\noffset_sigma_interact_acc68 <- function(singles_data) {\n  glm(accuracy_68 ~ measurement_error + offset_magnitude + \n        measurement_error * offset_magnitude, \n      data = singles_data, family = binomial)\n}\n\n\noffset_only_acc95 <- function(singles_data) {\n  glm(accuracy_95 ~ offset_magnitude, data = singles_data, \n      family = binomial)\n}\n\nsigma_only_acc95 <- function(singles_data) {\n  glm(accuracy_95 ~ measurement_error, data = singles_data, \n      family = binomial)\n}\n\noffset_sigma_acc95 <- function(singles_data) {\n  glm(accuracy_95 ~ measurement_error + offset_magnitude, data = singles_data, \n      family = binomial)\n}\n\noffset_sigma_interact_acc95 <- function(singles_data) {\n  glm(accuracy_95 ~ measurement_error + offset_magnitude + \n        measurement_error * offset_magnitude, \n      data = singles_data, family = binomial)\n}\n\n## Note to future self: some smart pivoting earlier on could have simplified this by half :)\n\n\n\n\nCode\n### Model execution\nsingle_cals_modelled <- single_cals_modelled %>%\n  mutate(\n    offset_only_acc68 = map(data, offset_only_acc68),\n    sigma_only_acc68 = map(data, sigma_only_acc68),\n    offset_sigma_acc68 = map(data, offset_sigma_acc68),\n    offset_sigma_interact_acc68 = map(data, offset_sigma_interact_acc68),\n    offset_only_acc95 = map(data, offset_only_acc95),\n    sigma_only_acc95 = map(data, sigma_only_acc95),\n    offset_sigma_acc95 = map(data, offset_sigma_acc95),\n    offset_sigma_interact_acc95 = map(data, offset_sigma_interact_acc95)\n  )\n\n\nNext, looked at model goodness params Then at model params themselves\nThen re-did the whole things with larger bins"
  },
  {
    "objectID": "posts/power_sensitivity_entry_3/index.html#model-fit-quality",
    "href": "posts/power_sensitivity_entry_3/index.html#model-fit-quality",
    "title": "Not enough power for sensitivity: 14C Power and Sensitivity (weeks 4-5?)",
    "section": "Model fit quality",
    "text": "Model fit quality\nThe first thing I did after getting the models run was to have a look at the quality parameters, the AIC and BIC estimates. If I were lucky, these would help me choose the best model for the job fast. As always, the reality was not as pleasant, but at least we got one interesting observation.\n\n\nCode\nsingle_cals_log_diagnostics %>%\n  mutate(\n    offset_pos = if_else(offset_pos, \"Pos offset\", \"Neg offset\")\n  ) %>%\n  filter(str_detect(model, \"68\")) %>%\n  ggplot(aes(x = target_year, y = AIC)) +\n  geom_line(aes(color = model)) +\n  facet_wrap(~offset_pos) +\n  theme_bw() +\n  scale_color_manual(values = c(\"darkblue\", \"blue\", \"steelblue\", \"gray\")) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.border = element_blank(),\n    strip.background = element_rect(\n     color=\"white\", fill=\"white\"\n     ),\n     text = element_text(family = \"Corbel\")\n    ) +\n  labs(\n    subtitle = \"68.2% HPD Area models: AIC values\",\n    x = \"Cal yrs BP\"\n    )\n\n\n\n\n\nWhen I too a look at the AIC values for the models built for different points in time, the first thing I noticed was the sigma_only model doing worse of all our models across the board (with AIC less is better). This was very, very reassuring - at least one expectation got fulfilled. What was a bit more challenging was that there was no clear winning model.\n\n\nCode\nsingle_cals_log_diagnostics %>%\n  mutate(\n    offset_pos = if_else(offset_pos, \"Pos offset\", \"Neg offset\")\n  ) %>%\n  filter(str_detect(model, \"95\")) %>%\n  ggplot(aes(x = target_year, y = AIC)) +\n  geom_line(aes(color = model)) +\n  facet_wrap(~offset_pos) +\n  theme_bw() +\n  scale_color_manual(values = c(\"darkblue\", \"blue\", \"steelblue\", \"gray\")) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.border = element_blank(),\n    strip.background = element_rect(\n     color=\"white\", fill=\"white\"\n     ),\n     text = element_text(family = \"Corbel\")\n    ) +\n  labs(\n    subtitle = \"95.4% HPD Area models: AIC values\",\n    x = \"Target Date (Cal BP)\"\n    )\n\n\n\n\n\nLooking at the 95.4% HPD ranges we have a similar story, but with a few twists. First of all, the AICs are lower than for the 68.2% HPD ranges. This makes sense: in the 68.2% case, we expect a lot more determinations to be off-target because of stochastic nature of measurement. This introduces way more noise, and thus the models have a harder time estimating. In the 95.4% case, the effects of non-stochastic effects (the ones we actually model on) is more decisive.\nBut taking one look at the graphs, the real interesting bit is elsewhere. Model quality is time-dependent - the further back you go, the better the models are at explaining variation in the data. As the big difference in calibration curve structure at those time scales is curve precision, we can expect that it had some impact on model explanatory power - perhaps by providing just a little less precise calibrations, thus further reducing stochastic effects of measurement uncertainty, and thus improving the explanatory power of the models."
  },
  {
    "objectID": "posts/power_sensitivity_entry_3/index.html#model-results-discussion",
    "href": "posts/power_sensitivity_entry_3/index.html#model-results-discussion",
    "title": "Not enough power for sensitivity: 14C Power and Sensitivity (weeks 4-5?)",
    "section": "Model results discussion",
    "text": "Model results discussion\nBase it off the research notes\nTwist - we lost the time dimension"
  },
  {
    "objectID": "posts/power_sensitivity_entry_3/index.html#try-again-bigger-bins",
    "href": "posts/power_sensitivity_entry_3/index.html#try-again-bigger-bins",
    "title": "Not enough power for sensitivity: 14C Power and Sensitivity (weeks 4-5?)",
    "section": "Try again: bigger bins",
    "text": "Try again: bigger bins\nBigger bins mean more simulations per block of time, mean more power to determine whats happening in the models (at the cost of chronological resolution). With the code already written, doing the change was simple enough.\n\n\nCode\n###Change to 10 bins\nsingle_cals_modelled_bin10 <- single_cals %>%\n  mutate (\n    offset_pos = if_else(offset_magnitude > 0, TRUE, FALSE),\n    offset_magnitude = if_else(offset_pos, \n                               offset_magnitude, offset_magnitude * -1),\n    # Changing negs on offset magnitude to have a consistent direction at downstream visualization.\n    binned_targets = ntile(target_year, 10)\n  ) %>%\n  select(-target_year)\n\n### ntile() wouldn't return the relevant target years, so it needs to be done as its own thing.\nsingle_cals_modelled_bin10 <- single_cals %>%\n  mutate(binned_targets = ntile(target_year, 10)) %>%\n  group_by(binned_targets) %>%\n  summarize( #Easy way to get years for the indiv bins, without too muchj headach\n    target_year = min(target_year)\n    ) %>%\n  inner_join(single_cals_modelled_bin10) %>% #Join the binned DF back in. No, I don't like this either!\n  group_by(offset_pos, target_year) %>%\n  nest()\n\n\n\n\nCode\n### Execute models\nsingle_cals_modelled_bin10 <- single_cals_modelled_bin10 %>%\n  mutate(\n    offset_only_acc68 = map(data, offset_only_acc68),\n    sigma_only_acc68 = map(data, sigma_only_acc68),\n    offset_sigma_acc68 = map(data, offset_sigma_acc68),\n    offset_sigma_interact_acc68 = map(data, offset_sigma_interact_acc68),\n    offset_only_acc95 = map(data, offset_only_acc95),\n    sigma_only_acc95 = map(data, sigma_only_acc95),\n    offset_sigma_acc95 = map(data, offset_sigma_acc95),\n    offset_sigma_interact_acc95 = map(data, offset_sigma_interact_acc95)\n  )\n\n\n\n\nCode\n### Extract the parameters\nmodel_names_bin10 <- colnames(single_cals_modelled_bin10)[4:11] ##These will tell us what results go with what model\n\nsingle_cals_log_results_bin10 <- data.frame()\nsingle_cals_log_diagnostics_bin10 <- data.frame()\n\nfor (model in model_names) {\n  ## This will get a little experimental - we are trying to create a big old table!\n  ## First, we thin down the DF to the model of interest\n  temp_results <- single_cals_modelled_bin10 %>%\n    select(1, 2, all_of(model)) %>% #All of used to address deprecation\n    rename(glm_list = 3) %>% #This rename allows map to work correctly\n    mutate(\n      logistic_results = map(glm_list, tidy), # Do the map and also take note of which model\n      model = model\n    ) %>%\n    select(-glm_list) %>% # Gets rid of the actual models (necessary for saving as csv)\n    unnest(logistic_results)# Unnests results\n  \n  temp_diagnostics <- single_cals_modelled_bin10 %>% ## Similar to above\n    select(1, 2, all_of(model)) %>%\n    rename(glm_list = 3) %>%\n    mutate(\n      logistic_diagnostics = map(glm_list, glance),\n      model = model\n    ) %>%\n    select(-glm_list) %>%\n    unnest(logistic_diagnostics)\n  \n  single_cals_log_results_bin10 <- rbind(single_cals_log_results_bin10, \n                                         temp_results)\n  single_cals_log_diagnostics_bin10 <- rbind(single_cals_log_diagnostics_bin10, \n                                       temp_diagnostics)\n\n}\n\n\n\n\nCode\nsingle_cals_log_results_bin10 %>%\n  filter(str_detect(model, \"interact\") & str_detect(term, \"Intercept\")) %>%\n  mutate(\n    offset_pos = if_else(offset_pos, \"Pos offset\", \"Neg offset\"),\n    model = if_else(str_detect(model, \"68\"), \"68.2% HPD\", \"95.4% HPD\")\n  ) %>%\n  ggplot(aes(x = target_year, y = estimate)) +\n  geom_ribbon(aes(ymin = estimate - std.error * 2, \n                  ymax = estimate + std.error * 2,\n                  fill = model), alpha = 0.2, color = \"grey75\") +\n  geom_line(aes(color = model), linetype = \"dotted\") +\n  facet_grid(cols = vars(offset_pos), rows = vars(model), scales = \"free\") +\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(),\n    strip.background = element_rect(\n     color=\"white\", fill=\"white\"\n     ),\n     text = element_text(family = \"Corbel\")\n    ) +\n  scale_fill_manual(values = c(\"steelblue\", \"darkblue\")) +\n  labs(\n    subtitle = \"Intercept estimates from the models containing an interaction term\",\n    x = \"Target Date (Cal BP)\"\n  )\n\n\n\n\n\nThe bigger bins provide a little bit more clarity on what we’re looking at. As far as the 68.2% HPD areas are concerned, we can see a trend towards higher intercept estimates. This suggests that we may be beginning to see the effects of the temporal trend in the data. However, this is only possible after we’ve sacrificed a lot of temporal resolution. In other words, to gain temporal resolution and evaluate the intercept, we would need way more simulations. How many is way more can be calculated now that we’ve got some estimates (and thus we can do the power analysis). Another option would be to ditch the bins and take a time series approach, hoping that this would give us a better handle on the trend. But before going into either of these options lets check out what happens if we take the other approach that came out of EDA: using the curve uncertainty as means of defining time-dependent factors. Which I shall do in the next installment."
  },
  {
    "objectID": "posts/power_sensitivity_entry_3/index.html#conclusion",
    "href": "posts/power_sensitivity_entry_3/index.html#conclusion",
    "title": "Not enough power for sensitivity: 14C Power and Sensitivity (weeks 4-5?)",
    "section": "Conclusion",
    "text": "Conclusion\nNot enough power for precision. Important bit - What changes in this characterization might be the baseline (intercept). This makes sense."
  },
  {
    "objectID": "posts/power_sensitivity_entry_3/index.html#how-the-modelling-got-done",
    "href": "posts/power_sensitivity_entry_3/index.html#how-the-modelling-got-done",
    "title": "Not enough power for sensitivity: 14C Power and Sensitivity (weeks 4-5?)",
    "section": "How the modelling got done",
    "text": "How the modelling got done\nThe plan involved a lot of models. To be specific four general model designs times however many bins times two to account for positive and negative offsets (after all, we cannot know if their impact on the data is symmetric in all cases and hence, I was reluctant to just use absolute values), times two again for the 68.2% and 95.4% HPD ranges of the simulated radiocarbon dates.\nI did the guts of the modelling with the purr and broom method from R4DS: compressed the binned data using nest() and then applied the purpose built model functions to the relevant data. Hence the whole modelling process was over in a matter of maybe a half-hour, including the time to remind myself how the code was meant to work 😋.\nPlease note: for those interested all the wrangling code is retained in the endless code chunks below\n\n\nCode\n### Set-up for fifty bins\nsingles_data_url <- \"https://raw.githubusercontent.com/pete-jacobsson/14C-power-sensitivity/main/simulation_results/singles_011_results.csv\" \nsingle_cals <- read_csv(url(singles_data_url))\n\n###Seperate pos from neg offsets and group things by cal curve \nsingle_cals_modelled <- single_cals %>%\n  mutate (\n    offset_pos = if_else(offset_magnitude > 0, TRUE, FALSE),\n    offset_magnitude = if_else(offset_pos, \n                               offset_magnitude, offset_magnitude * -1),\n    # Changing negs on offset magnitude to have a consistent direction at downstream visualization.\n    binned_targets = ntile(target_year, 50)\n  ) %>%\n  select(-target_year)\n\n### ntile() wouldn't return the relevant target years, so it needs to be done as its own thing.\nsingle_cals_modelled <- single_cals %>%\n  mutate(binned_targets = ntile(target_year, 50)) %>%\n  group_by(binned_targets) %>%\n  summarize( #Easy way to get years for the indiv bins, without too muchj headach\n    target_year = min(target_year)\n    ) %>%\n  inner_join(single_cals_modelled) %>% #Join the binned DF back in. No, I don't like this either!\n  group_by(offset_pos, target_year) %>%\n  nest()\n\n\n\n\nCode\n### Build the model functions\noffset_only_acc68 <- function(singles_data) {\n  glm(accuracy_68 ~ offset_magnitude, data = singles_data, \n      family = binomial)\n}\n\nsigma_only_acc68 <- function(singles_data) {\n  glm(accuracy_68 ~ measurement_error, data = singles_data, \n      family = binomial)\n}\n\noffset_sigma_acc68 <- function(singles_data) {\n  glm(accuracy_68 ~ measurement_error + offset_magnitude, data = singles_data, \n      family = binomial)\n}\n\noffset_sigma_interact_acc68 <- function(singles_data) {\n  glm(accuracy_68 ~ measurement_error + offset_magnitude + \n        measurement_error * offset_magnitude, \n      data = singles_data, family = binomial)\n}\n\n\noffset_only_acc95 <- function(singles_data) {\n  glm(accuracy_95 ~ offset_magnitude, data = singles_data, \n      family = binomial)\n}\n\nsigma_only_acc95 <- function(singles_data) {\n  glm(accuracy_95 ~ measurement_error, data = singles_data, \n      family = binomial)\n}\n\noffset_sigma_acc95 <- function(singles_data) {\n  glm(accuracy_95 ~ measurement_error + offset_magnitude, data = singles_data, \n      family = binomial)\n}\n\noffset_sigma_interact_acc95 <- function(singles_data) {\n  glm(accuracy_95 ~ measurement_error + offset_magnitude + \n        measurement_error * offset_magnitude, \n      data = singles_data, family = binomial)\n}\n\n## Note to future self: some smart pivoting earlier on could have simplified this by half :)\n\n\n\n\nCode\n### Model execution\nsingle_cals_modelled <- single_cals_modelled %>%\n  mutate(\n    offset_only_acc68 = map(data, offset_only_acc68),\n    sigma_only_acc68 = map(data, sigma_only_acc68),\n    offset_sigma_acc68 = map(data, offset_sigma_acc68),\n    offset_sigma_interact_acc68 = map(data, offset_sigma_interact_acc68),\n    offset_only_acc95 = map(data, offset_only_acc95),\n    sigma_only_acc95 = map(data, sigma_only_acc95),\n    offset_sigma_acc95 = map(data, offset_sigma_acc95),\n    offset_sigma_interact_acc95 = map(data, offset_sigma_interact_acc95)\n  )\n\n\n\n\nCode\n### Now get the numbers out of the models - aka the actual hard bit XP\n\nmodel_names <- colnames(single_cals_modelled)[4:11] ##These will tell us what results go with what model\n\nsingle_cals_log_results <- data.frame()\nsingle_cals_log_diagnostics <- data.frame()\n\nfor (model in model_names) {\n  ## This will get a little experimental - we are trying to create a big old table!\n  ## First, we thin down the DF to the model of interest\n  temp_results <- single_cals_modelled %>%\n    select(1, 2, all_of(model)) %>% #All of used to address deprecation\n    rename(glm_list = 3) %>% #This rename allows map to work correctly\n    mutate(\n      logistic_results = map(glm_list, tidy), # Do the map and also take note of which model\n      model = model\n    ) %>%\n    select(-glm_list) %>% # Gets rid of the actual models (necessary for saving as csv)\n    unnest(logistic_results)# Unnests results\n  \n  temp_diagnostics <- single_cals_modelled %>% ## Similar to above\n    select(1, 2, all_of(model)) %>%\n    rename(glm_list = 3) %>%\n    mutate(\n      logistic_diagnostics = map(glm_list, glance),\n      model = model\n    ) %>%\n    select(-glm_list) %>%\n    unnest(logistic_diagnostics)\n  \n  single_cals_log_results <- rbind(single_cals_log_results, temp_results)\n  single_cals_log_diagnostics <- rbind(single_cals_log_diagnostics, \n                                       temp_diagnostics)\n\n}"
  },
  {
    "objectID": "posts/power_sensitivity_entry_3/index.html#model-results-hints-at-the-models-but-not-enough-power.",
    "href": "posts/power_sensitivity_entry_3/index.html#model-results-hints-at-the-models-but-not-enough-power.",
    "title": "Not enough power for sensitivity: 14C Power and Sensitivity (weeks 4-5?)",
    "section": "Model results: hints at the models, but not enough power.",
    "text": "Model results: hints at the models, but not enough power.\nWhile model quality parameters provided some interesting insights, they did not bring us closer to our goal of choosing a model that would be good for predicting whether radiocarbon determinations subject to certain magnitudes of systematic offset will produce accurate calibrated date ranges. So the other approach was to look at what the actual model results told us. Now, one thing that was clear right off the bat was that the “sigma_only” models wouldn’t cut it: they did not perform well for the 68.2% HPD ranges, and also failed to take offset magnitude into the account (the thing we want to be predicting). Second, the offset-only models also wouldn’t do, because we know from preliminary visualizations that measurement precision does have an impact on model accuracy and the point is to quantify how much of an impact and under what conditions (not to mention, in practice, measurement precision might be the only thing that the lab teams might have some control over).\nThis leaves two models to choose from. The one with the interaction between measurement precision and offset magnitude, and the one without the interaction term. Both have comparable AIC and BIC, so it will be the model parameters that make the difference between the two. So lets visualize those:\n\n\nCode\nsingle_cals_log_results %>%\n  filter(str_detect(model, \"offset_sigma\") & !str_detect(term, \"Intercept\") &\n           offset_pos) %>%\n  filter(!(target_year %in% c(7019, 7751, 9076, 11091) & \n             str_detect(model, \"95\"))) %>%  ## This removes extreme values of the 95% model in the visualizations\n  mutate(\n    hpd_area = if_else(str_detect(model, \"68\"), \"68.2% HPD\", \"95.4% HPD\"),\n    model = str_remove(model, \"_acc68|_acc95\")\n  ) %>%\n  ggplot(aes(x = target_year, y = estimate)) +\n  geom_ribbon(aes(ymin = estimate - std.error * 2, \n                  ymax = estimate + std.error * 2,\n                  fill = term), alpha = 0.2, color = \"grey75\") +\n  geom_line(aes(color = term), linetype = \"dotted\") +\n  facet_grid(cols = vars(model), rows = vars(hpd_area), scales = \"free\") +\n  ylim(c(-2, 2)) +\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(),\n    strip.background = element_rect(\n     color=\"white\", fill=\"white\"\n     ),\n     text = element_text(family = \"Corbel\")\n    ) +\n  scale_fill_manual(name = \"Parameter\", values = c(\"steelblue\", \"grey50\", \"darkblue\")) +\n  scale_color_discrete(name = \"Parameter\") +\n  labs(\n    subtitle = \"Estimates for the models without and with an interaction term\",\n    x = \"Target year (cal BP)\",\n    caption = \"Please note: visualization only contains model with positive offsets\"\n  )\n\n\n\n\n\nThere are a few things going on here. The first is that the uncertainty around the estimates for the 95% HPD areas is much greater than the uncertainties for the models for the 68% HPD areas. This is as expected: the 95.4% HPD areas have a much higher ratio of things being “on target” and hence the logistic regression has far fewer “failures” to work with - and thus it is harder to get good precision.\nThe second item of interest is that the model without the interaction term produces more precise estimates and that the interaction term is small and very close to zero. Without any other information, this could lead us to choose the non-interaction model as providing better predictions.\nHowever, there is a third item to consider: the sign on the error parameters under the two models. In the non-interaction models the sign tends to be positive, implying that measurements with greater measurement error estimates (i.e. the less precise ones) give us more accurate results. Point. However, this is not true - if we have a situation where there is no systematic offset from the mean of the calibration curve, then a measurement with a precision better than that of the calibration curve will provide the accurate result almost all of the time. Precision only becomes detrimental with offsets and hence the interaction term becomes important for producing a realistic model of the physical situation we are dealing with.\nHaving said that both models still miss one point from the EDA: they do not show any meaningful change through time. When I first saw that, I though, hey, it might just be the case that the difference lies in the baseline accuracy - that is, in the intercept parameter. Alas, I was disappointed:\n\n\nCode\nsingle_cals_log_results %>%\n  filter(str_detect(model, \"interact\") & str_detect(term, \"Intercept\")) %>%\n  filter(!(target_year %in% c(7019, 7751, 9076, 11091) & \n             str_detect(model, \"95\"))) %>%  ## This removes extreme values of the 95% model in the visualizations\n  mutate(\n    offset_pos = if_else(offset_pos, \"Pos offset\", \"Neg offset\"),\n    model = if_else(str_detect(model, \"68\"), \"68.2% HPD\", \"95.4% HPD\")\n  ) %>%\n  ggplot(aes(x = target_year, y = estimate)) +\n  geom_hline(yintercept = 0) +\n  geom_ribbon(aes(ymin = estimate - std.error * 2, \n                  ymax = estimate + std.error * 2,\n                  fill = offset_pos), alpha = 0.2, color = \"grey75\") +\n  geom_line(aes(color = offset_pos), linetype = \"dotted\") +\n  facet_grid(cols = vars(offset_pos), rows = vars(model), scales = \"free\") +\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(),\n    strip.background = element_rect(\n     color=\"white\", fill=\"white\"\n     ),\n     text = element_text(family = \"Corbel\")\n    ) +\n  scale_fill_manual(values = c(\"steelblue\", \"darkblue\")) +\n  labs(\n    subtitle = \"Intercept estimates from the models containing an interaction term\",\n    x = \"Target Date (Cal BP)\"\n  )\n\n\n\n\n\nWhile the estimates did bounce up and down a bit, had any pattern been there, it was lost in the uncertainties. I lacked the power to detect the increase in accuracy through time at this resolution. So I went off and changed the bins."
  },
  {
    "objectID": "posts/power_sensitivity_entry_4/index.html",
    "href": "posts/power_sensitivity_entry_4/index.html",
    "title": "Not enough power for sensitivity: Compromises (weeks 6-10?)",
    "section": "",
    "text": "Hello again! Well, this post was on the slow side in the coming… A few things happened. One of them was that I walked into a Microsoft PL300 run without having touched Power BI beforehand - lets say that it was (at least) a demanding route of advance and I promptly had to limit time on other projects. Of course this coincided with the time when a lot of time was what I needed to push this sensitivity project along. So what happened?"
  },
  {
    "objectID": "posts/power_sensitivity_entry_4/index.html#wandering-estimates",
    "href": "posts/power_sensitivity_entry_4/index.html#wandering-estimates",
    "title": "Not enough power for sensitivity: Compromises (weeks 6-10?)",
    "section": "Wandering estimates",
    "text": "Wandering estimates\nWhen we left off, the aim was to shift the planned approach from one that tried to build different models for different points in time and then using those to predict if the radiocarbon measurements, given a certain offset magnitude, are going to be off-target or not. As an alternative, the plan was to include calibration curve uncertainty and check how it impacts model accuracy.\nAt first things were coming along nicely. I’ve set up a nice collection of models and had a look at the various model quality metrics.\n\n\nCode\n## Get model diagnostics\n### Set-up for fifty bins\nmodel_diags_url <- \"https://raw.githubusercontent.com/pete-jacobsson/14C-power-sensitivity/main/model_results/single_cals_curve_uncert_diag.csv\" \nsingle_cals_curve_uncert_diag <- read_csv(url(model_diags_url))\n\n\n\n\nCode\nsingle_cals_curve_uncert_diag %>%\n  mutate(\n    offset_direction = if_else(is_pos, \"Positive\", \"Negative\"),\n    hpd_area = if_else(str_detect(model, \"68\"), \"68.2% HPD Area\", \"95.4% HPD Area\"),\n    model = str_remove_all(model, \"_acc\\\\d{2}\")\n  ) %>%\n  ggplot(aes(x = model, y = AIC)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\", color = \"grey40\") +\n  facet_grid(rows = vars(hpd_area), cols = vars(offset_direction), scales = \"free\") +\n  labs(\n    x = \"Model\",\n    subtitle = \"AIC of different models that include curve uncertainty\"\n  ) +\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.border = element_blank(),\n    axis.line = element_line(colour = \"grey50\", size = 0.5),\n    #legend.position = \"none\",\n    strip.background = element_rect(\n     color=\"white\", fill=\"white\"\n     ),\n     text = element_text(family = \"Corbel\"),\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = 9)\n    )\n\n\n\n\n\nEncouraged by the results, I proceeded to check the model fit to the input data. It’s here when things got slow going. As I was using a custom model, inputing the spec into the ggplot smooth function was not going to work. I needed to build a set of my own functions to do the task. It took a little bit of Stack Exchanging to come up with a plan and also the jtools package came in very handy - in particular it contained the function make_predictions(), which can be used to build a set of model predictions, given an input model.\n\n\nCode\npredict_model_results_osc <- function(offset_dir, hpd_area, model_name,\n                                      variable, variable_range) {\n  ##Function to simplify prediction generation\n  ##Takes on offset_dir and hpd_area, which simplify the results table,\n  ##Takes on variable name as string and variable range as a sequence\n  ##Assumes offset dir, hpd_area, and variable name provided are all correct\n  \n  \n  offset_curve_sigma_os <- readRDS(\"single_cals_curve_uncert_regr.rds\") %>%\n    filter(str_detect(model, model_name)) %>%\n    mutate(is_pos = if_else(is_pos, \"positive\", \"negative\")) %>% ## Change type for filtering\n    filter(str_detect(is_pos, offset_dir) & str_detect(model, hpd_area))\n  \n  predicted_results <- make_predictions(offset_curve_sigma_os$model_results[[1]],\n                                        pred = variable,\n                                        pred.values = variable_range)\n  \n  predicted_results\n}\n\n# predict_model_results_osc(offset_dir = \"pos\", hpd_area = \"68\",\n#                           model_name = \"offset_curve_interact\",\n#                           variable = \"offset_magnitude\", \n#                           variable_range = seq(0, 50))\n\n\nplot_simulation_results_osc <- function(range_to_plot, hpd_area, variable,\n                                        rounding, xlab) {\n  ##This function plots the simulation results for the purposes of the results graphing\n  \n  single_cals_curve_uncert_mod <- read_csv(\n    url(\"https://raw.githubusercontent.com/pete-jacobsson/14C-power-sensitivity/main/single_cals_w_curve_uncert.csv\")) %>%\n  mutate(sigma_curve_uncert = sqrt((measurement_error^2) + (curve_uncert^2)))\n  \n  single_cals_curve_uncert_mod %>%\n    rename(variable_to_plot = variable, hpd_to_plot = hpd_area) %>%\n    filter(variable_to_plot >= range_to_plot[1] & \n             variable_to_plot <= range_to_plot) %>%\n    mutate(\n      variable_to_plot = plyr::round_any(variable_to_plot, rounding)\n    ) %>%\n    select(variable_to_plot, hpd_to_plot) %>%\n    group_by(variable_to_plot) %>%\n    summarize(\n      ratio_accurate = mean(hpd_to_plot)\n    ) %>%\n    ggplot(aes(x = variable_to_plot, y = ratio_accurate)) +\n    geom_bar(stat = 'identity', fill = \"steelblue\") +\n    theme_bw() +\n    theme(\n      panel.grid.minor = element_blank(),\n      panel.grid.major = element_blank(),\n      panel.border = element_blank(),\n      axis.line = element_line(colour = \"grey50\", linewidth = 0.5),\n      text = element_text(family = \"Corbel\")#,\n      #axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = 9)\n      ) +\n    labs(\n      y = \"Ratio accurate\",\n      x = xlab\n    )\n  \n}\n\n# plot_simulation_results_osc(range_to_plot = c(0, 50), hpd_area = \"accuracy_68\",\n#                             variable = \"offset_magnitude\", rounding = 2,\n#                             xlab = \"Offset magnitude\")\n\n\nplot_model_predictions_ocs <- function(offset_dir, hpd_area, model_name, variable, \n                                       range, rounding, xlab) {\n  ## This function wraps predict_model_results and plot_simulation_results and plots prediction over simulation.\n  ## parameters and assumptions as per underpinning functions.\n  plot <- plot_simulation_results_osc(range_to_plot = range, hpd_area = hpd_area,\n                                      variable = variable, rounding = rounding,\n                                      xlab = xlab)\n  \n  hpd_for_predict = str_extract(hpd_area, \"\\\\d{2}\") ## predict_model_results only takes two-digit numbers as input here (for filtering) - this extracts from HPD area above\n  \n  predicts_tab <- predict_model_results_osc(offset_dir = offset_dir,\n                                            hpd_area = hpd_for_predict, \n                                            model_name = model_name,\n                                            variable = variable, \n                                            variable_range = seq(range[1],\n                                                                 range[2]))\n  predicts_tab <- predicts_tab %>%\n    rename(\"variable_to_plot\" = variable, ratio_accurate = 1) ##Dovetail the variable names for the plot at the iteration where interactions start getting explored\n  \n  plot <- plot +\n    geom_line(data = predicts_tab)\n  \n  plot\n}\n\n# plot_model_predictions_ocs(offset_dir = \"pos\", hpd_area = \"accuracy_68\",\n#                            model_name = \"act_oc_sc_acc\",\n#                            variable = \"offset_magnitude\",\n#                            range = c(0, 50), rounding = 2.5,\n#                            xlab = \"Offset magnitude\")\n\n\nQuite cheerful I proceeded to plot the results. They came back as pictured on the associated pictures: not matching the model very well. In particular the measurement error estimates seemed to be very far off what they’d need to be to match the data.\n\n\nCode\noc_sc_plot <- plot_model_predictions_ocs(offset_dir = \"neg\", \n                                       hpd_area = \"accuracy_95\",\n                                       variable = \"measurement_error\",\n                                       model_name = \"os_cs\",\n                                       range = c(8, 32), rounding = 2,\n                                       xlab = \"Measurement error\") + \n  labs(\n    title = \"Ratio accurate determinations in simulated sample and the fitted regression line\",\n    caption = \"Define: 'Model does not match the data'\"\n  )"
  },
  {
    "objectID": "posts/power_sensitivity_entry_4/index.html#if-in-doubt-combine",
    "href": "posts/power_sensitivity_entry_4/index.html#if-in-doubt-combine",
    "title": "Not enough power for sensitivity: Compromises (weeks 6-10?)",
    "section": "If in doubt: combine!",
    "text": "If in doubt: combine!\nAt this stage I started suspecting that this has something to do with how curve uncertainty estimate affects the model. Long and short, it looked like the models built using both curve uncertainty data and the measurement error data might have been confounding the two effects. I decided to address this by a trick: combining the measurement error and the curve uncertainty into a single parameter. While not a pretty solution, in practice, for most samples, we would have some expectation of age prior to measurement and thus we could make a reasonable guess on the magnitude of curve uncertainty (indeed better than the guess as to what measurement precision the instruments would return). With this data in tow there would thus be the possibility of predicting model accuracy.\nThis left me with two possible models to take to STAN: one with an interaction between the two parameters, one without such interaction.\nThe intercepts were comparable within the models, though the model with interactions was giving meaningless predictions if we calculated the values at zero offset and uncertainty (basically plugging the intercept only into the logistic equation).\n\n\nCode\noffset_curve_sigma_comb_interact_params <- readRDS(\"single_cals_curve_uncert_regr.rds\") |>\n  filter(str_detect(model, \"comb\")) |>\n  mutate(model_results = map(model_results, tidy)) |>\n  unnest(cols = c(model_results))\n\n\noffset_curve_sigma_comb_interact_params |>\n  filter(str_detect(term, \"Interce\")) |>\n  select(is_pos, model, estimate, std.error) |>\n  mutate(\n    acc_zero = 1 / (1 + exp(- estimate)),\n    acc_zero_min = 1 / (1 + exp(- (estimate - 2 * std.error))),\n    acc_zero_max = 1 / (1 + exp(- (estimate + 2 * std.error))),\n    ) |>\n    mutate(\n      offset_dir = if_else(is_pos, \"Positive\", \"Negative\"),\n      hpd_area = str_extract(model, \"\\\\d{2}\"),\n      hpd_area = str_c(hpd_area, \"% HPD Area\"),\n      model = if_else(str_detect(model, \"intera\"), \"Interaction\", \"No interaction\")\n    ) |>\n    select(offset_dir, model, hpd_area, acc_zero, \n           acc_zero_min, acc_zero_max) |>\n  ggplot(aes(x = model, y = acc_zero)) +\n  geom_point() +\n  geom_linerange(aes(ymax = acc_zero_max, ymin = acc_zero_min)) +\n  facet_grid(rows = vars(hpd_area), cols = vars(offset_dir)) +\n  ylim(c(0, 1)) +\n  theme_bw() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.border = element_blank(),\n    axis.line = element_line(colour = \"grey50\", size = 0.5),\n    #legend.position = \"none\",\n    strip.background = element_rect(\n     color=\"white\", fill=\"white\"\n     ),\n     text = element_text(family = \"Corbel\"),\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = 9)\n    ) +\n  labs(\n    x = \"\",\n    y = \"Ratio accurate\",\n    title = \"Predicted ratio of accurate observations under no offset and no error\"\n  )\n\n\n\n\n\nWhich we’ll cover in the next installment… with three weeks of an intensive French course ahead I’ve got no idea when the next installment will be though!"
  },
  {
    "objectID": "posts/data_analyst_descript/index.html",
    "href": "posts/data_analyst_descript/index.html",
    "title": "What’s in a generic data analyst job description?",
    "section": "",
    "text": "As some of you may know, I am looking for a job, with something in data analytics being one of the possibilities. Now, there is a lot of advice out there when it comes to job hunting and the big one is tailoring your CV to whatever job you’re applying to. Indeed, with the advent of easy-to-access LLMs there are now tools to automate the process (Teal comes to mind). However, there is also another big lot of advice and that is that a lot of jobs never make it to the job board and that it is often best to work with a recruiter. Which also includes a CV, but this time with no hints as to how to tailor it.\nSo I thought, how about working out what the average, generic data analyst job description looks like - what are the skills and tasks that come up most often. Given that I was thinking of going for a data analyst role, it was only fitting to approach the problem using data. Here is what I found in the process."
  },
  {
    "objectID": "posts/data_analyst_descript/index.html#how-to-get-the-data-aka-the-method-section",
    "href": "posts/data_analyst_descript/index.html#how-to-get-the-data-aka-the-method-section",
    "title": "What’s in a generic data analyst job description?",
    "section": "How to get the data? (aka the method section)",
    "text": "How to get the data? (aka the method section)\nFrom the get go the big problem was how to get the data. Now in principle I could sit down in front of Indeed, Glassdoor, or some other Linked In and start going through adverts for where I live - which happens to be western Switzerland. Now, this approach has a couple big drawbacks, the biggest being that it is slow. The second biggest being that it also introduces a lot of subjectivity. A slight variation would have been to paste the job descriptions into a file and then try to process them somehow - still slow and now I have a massive NLP exercise to deal with. Instead of doing all those things, I approached the problem from a more technical angle (consult the figure for the TLDR version).\nThe key was to get API access to job descriptions. There are a few services out there that offer that, however quite a few of them are paywalled. Still, where there is will to spend more than five minutes googling there is hope and thus I came across the Adzuna website. After a quick API refresher I was able to get a whole collection of recent job descriptions… except the API never had any job descriptions, only links to posts and snippets of the first however many words or symbols of a description. At that stage I quit for the day.\nComing back the next day, I approached the issue from a different angle. While the API didn’t give full job descriptions it did supply links to individual pages. That’s how the new plan got hatched: web scraping. After a few more refreshers (and a little while digging through Adzuna’s CSS selectors) I managed to get the scraping function to work. There was one hiccup in that some of the links were for external sites, but this was easy to determine and filter away from the links themselves.\nThus the last remaining task was to deal with extracting the information from job descriptions. Now, a year earlier that would have been messy to say the least. However, since then, accessible LLMs became a thing. Coming off the back of Isa Fulford’s and Andrew Ng’s course I used GPT 3.5 Turbo to do the work for me. The prompt of course requires some iterating, but the summarizing capacity of the LLM meant that the job descriptions could be iterated over fast. The workflow as a whole took a couple hours to execute and yielded 159 job descriptions posted in early July 2023. In principle the process could be faster, however I made sure to include multiple “sleeps” in the process - after all Open AI API can only be called so many times per minute and I also wanted to limit the impact of the webscrape on Adzuna to something akin to a well-caffeinated human clicking too much. With the data in hand visualization was all that was left.\nI used R throughout, with a smattering of Python (wanted to check how R Studio’s Python side would behave). All code is attached at the end of the post and, if you’re interested, the actual work log can be found on my Github (apologies if some words might have evaded the profanity removal process prior to publishing)."
  },
  {
    "objectID": "posts/data_analyst_descript/index.html#results",
    "href": "posts/data_analyst_descript/index.html#results",
    "title": "What’s in a generic data analyst job description?",
    "section": "Results",
    "text": "Results\nAs you can see from the graph below there are some expected and some less-expected outcomes from the exercise.\n\n\nCode\nskills_df_url <- \"https://raw.githubusercontent.com/pete-jacobsson/what_makes_a_data_analyst/main/skills_df.csv\" \nskills_df <- read_csv(url(skills_df_url))\n\nskills_df |>\n  head(28) |>\n  mutate(\n    term = as_factor(term)\n  ) |>\n  filter(term != \"Analytics\") |> ### This category is largest - which makes sense, but we know it from the job description :)\n  ggplot(aes(x = fct_infreq(term), y = count)) +\n  geom_bar(stat = 'identity', fill = \"steelblue\", color = \"white\") +\n  geom_text(aes(label = count), nudge_y = 2, size = 3) +\n  coord_flip() +\n  scale_x_discrete(limits = rev) +\n  theme_bw() +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.border = element_blank(),\n    axis.line = element_line(colour = \"grey50\", size = 0.5),\n    text = element_text(family = \"Corbel\")#,\n    #axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = 9)\n    ) +\n  labs(\n    x = \"GPT-detected keyword\",\n    y = \"Count\"\n  )\n\n\n\n\n\nLet’s start with the obvious and the expected. First off the bat, in the sample of 159 job descriptions, keywords relating to core soft skills were most common, with GPT detecting both interpersonal skills and communication skills in over eighty descriptions each. These are followed by business awareness. Python and knowledge of SQL come up as the two core hard skills and languages follow an expected pattern, with English in the fourth position, and German and French trailing. German and French are to be expected, given that I was scraping a Swiss job board. Last, Excel is present as the third most in demand tech skill. While this may come across as a surprise to some, in the core data analytics space this can be expected - after all, there is Excel and then there is Excel with all the added VBA bells and whistles and the tool still sees heavy usage (there is a reason why there are R packages dedicated to generating Excel files).\nNow let’s talk about the surprises. The first major item off the bat is the low value assigned to curiosity - less than ten of the job descriptions noted curiosity as a required or desirable characteristic for a data analyst post. As a friend of mine pointed out, at face value this makes no sense in a job which is all about digging deep into research questions. Yes, you can do it without a curious mind, but chances are you will not enjoy the job and you will not reach your full potential. Now, it is possible that curiosity features low on the requirements list because it is an untestable characteristic. After all, at the level of the initial application anyone can claim that they have an endless streak of curiosity and most of the time they will believe it. What would be more concerning is if the hiring teams don’t find curiosity an important characteristic in candidates and fail to evaluate for it throughout the application process.\nLinked to this is the low ranking of both research experience and data management skills. All the same arguments repeated above apply: the core of an analysts job should be doing research, hence I would have expected research experience to score more hits. Likewise, data management is essential to keeping the pipelines healthy and preventing bad stuff from happening. As such we could expect it to show up more often, even if not as an essential part of the job.\nThe other item that surprised me was the continuing dominance of scripting languages over the out-of-the-box tools. Python is more sought after than Power BI and R is more sought after than Tableau. Hence, in this particular sample, knowing how to code goes a longer way towards fitting the job description than knowing how to use code-free data analytics tools. Furthermore, there are some other skills in there which suggest that hard technical skills, such as Cloud operations and API usage, may be sought after as part of a generic “data analyst” package. In other words, doing the Microsoft PL300 alone would be insufficient to match a job description generated from these findings only. Now, this may be a quirk of the search algorithm, but it does seem to demonstrate that in mid-2023 code is still important."
  },
  {
    "objectID": "posts/data_analyst_descript/index.html#conclusions",
    "href": "posts/data_analyst_descript/index.html#conclusions",
    "title": "What’s in a generic data analyst job description?",
    "section": "Conclusions",
    "text": "Conclusions"
  },
  {
    "objectID": "posts/data_analyst_descript/index.html#code",
    "href": "posts/data_analyst_descript/index.html#code",
    "title": "What’s in a generic data analyst job description?",
    "section": "Code",
    "text": "Code\n\n\nCode\n## Reproducing results will require the following libraries:\nlibrary(httr)\nlibrary(tidyverse)\nlibrary(jsonlite)\nlibrary(rvest)\nlibrary(reticulate)\n\n\n\nAPI functions\n\n\nCode\n##This chunk is concerned with getting the API function working\n\n## Write a function to get any given page. After that I can just iterate while there are still jobs to find :)\n\nget_adzuna_api <- function(api_id, api_key, page, \n                           key_words = c(\"data\", \"analyst\"), country = \"ch\") { #API ID and key are variables, so that they can remain secret\n  \n  key_words <- stringr::str_c(key_words[1], \"%20\", key_words[2]) #Can only take two key words. All needed for most data professions\n  api_call_string <- stringr::str_c(\"http://api.adzuna.com/v1/api/jobs/\", \n                                    country, \"/search/\", page, \"?app_id=\", \n                                    api_id, \"&app_key=\", api_key,\n                                    \"&results_per_page=20&what=\", key_words,\n                                    \"&max_days_old=730&salary_include_unknown=1&content-type=application/json\")\n  api_return <- GET(api_call_string)\n  api_return_text <- content(api_return, \"text\")\n  api_return_df <- jsonlite::fromJSON(api_return_text)\n  \n  api_return_df$results\n}\n\n\n\n\nCode\n## Test the function\n## Note that you will need \ntest <- get_adzuna_api(\n  \"my_adzuna_id\", \n  \"my_adzuna_token\", 2) ##Requires an ID and key for reproduction. \n\n\ntest$location$display_name\n\n\n\n\nCode\n## This code block is about removing unwanted columns from the API response and renaming other ones.\n## Functionalizing for easier integration into the automated pipeline downstream\n\nclean_adzuna_response <- function(adzuna_response) {\n  response_cleaned <- data.frame(\n    date_posted = adzuna_response$created,\n    job_title = adzuna_response$title,\n    company = adzuna_response$company$display_name,\n    location = adzuna_response$location$display_name,\n    website = adzuna_response$redirect_url\n  ) |>\n    filter(str_detect(website, \"details\")) #Adzuna links quite a few instances of external job boards\n  \n  \n  response_cleaned\n}\n\n\n\n\nCode\n##Test clean_adzuna_response\ntest2 <- clean_adzuna_response(test)\nwrite_csv(test2, \"test_returns.csv\")\n\n\n\n\nWeb scrape functions\n\n\nCode\n##Lets get a URL to visit\n##Note: the website links are out of date at time of publication.\n##For reproduction needs an up-to-date test.returns.csv\ntest2 <- read_csv(\"test_returns.csv\")\ntest2$website[4]\n\n\n\n\nCode\n### Parsing function to make loops easier to work with.\n\nparse_adzuna_data <- function (adzuna_http){\n  adzuna_html_response <- read_html(adzuna_http) #Get the website\n  adzuna_body <- adzuna_html_response |> #Extract the body of the job ad\n    html_nodes(\".adp-body\") |>\n    html_text()\n  \n  adzuna_body\n}\n\n\n\n\nCode\n##Test the function: run for the length of a whole DF\n##Use for loop to ensure spacing between website hits\n##Pass if test_vector contains 15 long extracted strings\n\n### Commented out - don't want to run loops hitting people's websites by accident :)\n# test_vector <- c()\n# \n# for (i in 1:nrow(test2)) { #test2 contains the readings from the newest set of previous tests\n#   adzuna_description <- parse_adzuna_data(test2$website[i])\n#   test_vector <- c(test_vector, adzuna_description)\n#   Sys.sleep(30)\n# }\n\nfor (item in test_vector) {\n  item_trunc <- str_trunc(item, 50)\n  print(item_trunc)\n}\n\n\n\n\nGPT 3.5 functions\n\n\nCode\n### Following the method from this course: https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/\nimport openai\nimport os\nimport time\n\n##COMMENT UP THIS CODE!!!\nwith open('/home/pete/Documents/gpt_key.txt') as t:\n  openai.api_key = t.readlines()[0].strip(\"\\n\")\n\n\ndef get_completion(prompt, model=\"gpt-3.5-turbo\", temperature = 0):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=temperature, # this is the degree of randomness of the model's output\n    )\n    return response.choices[0].message[\"content\"]\n\n\n\n\nCode\n###Test connection\nget_completion(\"Can you please tell me if I'm connected to GPT?\")\n\n\n\n\nCode\n##Get a return to work with for dev\ntest3 <- parse_adzuna_data(test2$website[2])\ntest3\n\n\n\n\nCode\n### Now lets pass a job description to Chat GPT and build a prompt\nadzuna_description = r.test3\nprint(adzuna_description)\n\n\n\n\nCode\ndef summarize_adzuna(adzuna_description):\n  adzuna_summary = get_completion(\n    \"Dear Chat GPT, could you please conduct the following actions on the test in square brackets: Identify all skills, characteristics and qualifications        required for the job and return them as itemized points in English. Split those into individual skills, with no more than two words per skill. Provide the return in JSON format please! [\" + adzuna_description +\"]\"\n  )\n  return(adzuna_summary)\n\n\n\n\nCode\ntest4 =  summarize_adzuna(r.test3)\nprint(test4)\n\n\n\n\nExecute API calls"
  }
]