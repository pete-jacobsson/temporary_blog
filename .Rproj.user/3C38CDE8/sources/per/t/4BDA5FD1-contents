---
title: "14C Power/Sensitivity notes"
author: "Piotr Jacobsson"
format: html
---

```{r}
library(tidyverse)
library(broom)
```


This document contains the research notes from the analysis of the simulations conducted using R and OxCal. This is a rough notebook that begins at the point when the project was re-initiated in June 2023 until completion

##Part 1: make sense of what is here and order things up
The first thing that I spotted was that much of the data files were sitting right in the main folder. So this will get ordered up - but I'll leave the Markdown files describing everything on top (I might need them lets see if past me had any brains worth of brain :P).

Next will be working out what the individual simulations are and what they cover - as well as taking some time to play with the Shiny App that I built to support the EDA. 
The thing looks much better now.

Lets have a look at the list of simulations we have.

```{r}
list_of_sims <- read_csv("sims_list.csv")
list_of_sims
```
There are a few things I could do here. I could start with the single calibrations and take it from there. Not sure what this would contribute though and I'd likely get bored. Also, there is the question of whether I want to keep working with test groups... Still could be fun refreshing what matters and chacking whether the calibration curve has anything to say about accuracy here.

For the wiggle-match dates, there are a few things that could be done. The first is to run models for the steep section and for the plateau and compare results. The interesting thing to do here would maybe be to use Stan while I am at it? The data sets are not that large and even if they turn out to be, I can truncate them. Yes, this could work.
Of course then there is the 300k project. It covers 1500 years, so the simulation density is 200 sims per year. Here I remember I wanted to go decade by decade, possibly with different regression models and start comparing. So we have options here for accuracy.
I might leave Sequences out at first pass.

Likewise I think it might be best to focus on accuracy early on - precision can come second. 

Oh and the Shiny will need a batch file. 

Had a quick look at Shiny. Next instalment should likely be some EDA with what I've got in the app and a decision if I want to maybe begin by completing that app.



# Single calibrations

## Part 2: Exploring the single radiocarbon dates with Shiny, and some prelim models

After some reflection I decided to start off with doing some model-based exploration of single calibrations. Using the Shiny App I was able to make some observations that helped.

1. The nominal accuracy was more or less OK for offsets up to about 15 - 25 14C years.
2. Greater measurement errors seem to mean more accurate dates
3. Measurements from different parts of the calibration curve may have different susceptibility to offsets. This seems to work different for positive and negative offsets.
4. Depending on the location on the calibration curve it may be that different offset directions have different effects.

Knowing this, the next step is to explore if any of these initial observations make it through exploratory modelling. The goal is to check is different parts of the calibration curve are comparable when it comes to effects of offset magnitudes and their interactions with measurement error magnitude.

```{r}
### Load the data

single_cals <- read_csv("simulation_results/singles_011_results.csv")

###Seperate pos from neg offsets and group things by cal curve 
single_cals_modelled <- single_cals %>%
  mutate (
    offset_pos = if_else(offset_magnitude > 0, TRUE, FALSE),
    offset_magnitude = if_else(offset_pos, 
                               offset_magnitude, offset_magnitude * -1),
    # Changing negs on offset magnitude to have a consistent direction at downstream visualization.
    binned_targets = ntile(target_year, 50)
  ) %>%
  select(-target_year)


single_cals_modelled <- single_cals %>%
  mutate(binned_targets = ntile(target_year, 50)) %>%
  group_by(binned_targets) %>%
  summarize( #Easy way to get years for the indiv bins, without too muchj headach
    target_year = min(target_year)
    ) %>%
  inner_join(single_cals_modelled) %>% #Join the binned DF back in. No, I don't like this either!
  group_by(offset_pos, target_year) %>%
  nest()


### Now lets specify a model to test if things work
# offset_only_acc68 <- function(singles_data) {
#   glm(accuracy_68 ~ offset_magnitude, data = singles_data, family = binomial(link = 'logit'))
# }

### After which we apply the model across the DF and check if everythign works as it should.


# single_cals_modelled <- single_cals_modelled %>%
#   mutate(offset_only_acc68 = map(data, offset_only_acc68))
# 
# summary(single_cals_modelled$offset_only_acc68[[1]])
##Comment out after testing to avoid clashes

## Great! Now build remaining models and apply to the DF. 

```

## Part 3 Fitting multiple logistic regressions
In this step I want to fit multiple logistic regressions to single calibrations I have the data frame ready, next step is to write the regression functions and apply them to the DF. 

```{r}
### Build functions to use with map
## Could have tried in-line but it would be very confusing with the number of parameters floating about. 

offset_only_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ offset_magnitude, data = singles_data, 
      family = binomial)
}

sigma_only_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_interact_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + 
        measurement_error * offset_magnitude, 
      data = singles_data, family = binomial(link = 'logit'))
}


offset_only_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ offset_magnitude, data = singles_data, 
      family = binomial(link = 'logit'))
}

sigma_only_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_interact_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + 
        measurement_error * offset_magnitude, 
      data = singles_data, family = binomial(link = 'logit'))
}


```

```{r}
### Now use map to apply the models across the data
single_cals_modelled <- single_cals_modelled %>%
  mutate(
    offset_only_acc68 = map(data, offset_only_acc68),
    sigma_only_acc68 = map(data, sigma_only_acc68),
    offset_sigma_acc68 = map(data, offset_sigma_acc68),
    offset_sigma_interact_acc68 = map(data, offset_sigma_interact_acc68),
    offset_only_acc95 = map(data, offset_only_acc95),
    sigma_only_acc95 = map(data, sigma_only_acc95),
    offset_sigma_acc95 = map(data, offset_sigma_acc95),
    offset_sigma_interact_acc95 = map(data, offset_sigma_interact_acc95)
  )

```


## Part 4: Unpack the ensted lists and re-organize the DFs

Right now I have one DF with a lot of nested models. These need unpacked and re-organized into a new DF if they are to be visualizable (which is our objective after all!).

Lets try to loop through the unpacking and then create two DFs - one for parameters and one for model quality indicators. That would also be a good point to save the DFs into csv files (so we don't need to keep re-building them each time we re-visit the data).

```{r}
model_names <- colnames(single_cals_modelled)[4:11]

single_cals_log_results <- data.frame()
single_cals_log_diagnostics <- data.frame()

for (model in model_names) {
  ## This will get a little experimental - we are trying to create a big old table!
  ## First, we thin down the DF to the model of interest
  temp_results <- single_cals_modelled %>%
    select(1, 2, all_of(model)) %>% #All of used to address deprecation
    rename(glm_list = 3) %>% #This rename allows map to work correctly
    mutate(
      logistic_results = map(glm_list, tidy), # Do the map and also take note of which model
      model = model
    ) %>%
    select(-glm_list) %>% # Gets rid of the actual models (necessary for saving as csv)
    unnest(logistic_results)# Unnests results
  
  temp_diagnostics <- single_cals_modelled %>%
    select(1, 2, all_of(model)) %>%
    rename(glm_list = 3) %>%
    mutate(
      logistic_diagnostics = map(glm_list, glance),
      model = model
    ) %>%
    select(-glm_list) %>%
    unnest(logistic_diagnostics)
  
  single_cals_log_results <- rbind(single_cals_log_results, temp_results)
  single_cals_log_diagnostics <- rbind(single_cals_log_diagnostics, 
                                       temp_diagnostics)

}


write_csv(single_cals_log_results, 
          "model_results/single_cals_log_results.csv")
write_csv(single_cals_log_diagnostics, 
          "model_results/single_cals_log_diagnostics.csv")

```



## Part 5: Start plotting what all the results mean :D

There are two things we are interested in here. First is, how do different parameters for different models vary with time. Second, what are the model quality indicators. I'll start by looking at the latter, more specifically at AIC and BIC values.

```{r}
single_cals_log_diagnostics <- read_csv("model_results/single_cals_log_diagnostics.csv")

```

```{r}

## Lets make some plots
single_cals_log_diagnostics %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "68")) %>%
  ggplot(aes(x = target_year, y = AIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos) +
  theme_bw() +
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  labs(x = "Cal yrs BP")


single_cals_log_diagnostics %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "68")) %>%
  ggplot(aes(x = target_year, y = BIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos, labeller = labeller(c("Pos offset", "Neg offset"))) +
  theme_bw() +
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    )) +
  labs(x = "Cal yrs BP")

single_cals_log_diagnostics %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "95")) %>%
  ggplot(aes(x = target_year, y = AIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos) +
  theme_bw()+
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    )) +
  labs(x = "Cal yrs BP")


single_cals_log_diagnostics %>%
   mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "95")) %>%
  ggplot(aes(x = target_year, y = BIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos) +
  theme_bw() +
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    )) +
  labs(x = "Cal yrs BP")


  
```

There are a few things to note here. 
First, as expected the Sigma-only model performs worst of the whole group.

Second, the remaining models are close to each other in performance, with the interaction model performing marginally better with respect to AIC and the offset only model performaing little better with respect to BIC

Third, the model AIC and BIC varies with the set of target dates - suggesting that the performance of logistic regression varies depending on the shape of the calibration curve at a given segment of time. Lets explore if we can find a pattern by adding in calibration curve information and drawing some scatter plots.

```{r}
## Can't just bin cal curve. Does not match well
## First get cal curve and min/max for the actual bins

cal_curve <- read_csv("intcal_20_interpolated.csv")

binned_sim_yrs <- single_cals %>%
  mutate(bin_yrs = ntile(target_year, 50)) %>%
  group_by(bin_yrs) %>%
  summarize(
    bin_end = max(target_year),
    target_year = min(target_year)
    
  )

## Now loop through each bin, and get BP sd for that group of years - a proxy for how variable the cal curve was (less = more plateau)
bp_sd <- c()

for (i in 1:nrow(binned_sim_yrs)) {
  binned_sd <- cal_curve %>%
    filter(CalBP >= binned_sim_yrs$target_year[i] & 
             CalBP <= binned_sim_yrs$bin_end[i]) %>%
    summarize(bin_sd = sd(BP)) %>%
    pull(bin_sd)
  
  bp_sd <- c(bp_sd, binned_sd)
}

binned_sim_yrs <- cbind(binned_sim_yrs, bp_sd) %>%
  select(target_year, bp_sd)

###Errors here. Re-work to ensure match!


inner_join(single_cals_log_diagnostics, binned_sim_yrs) %>%
  filter(str_detect(model, "68")) %>%
  ggplot(aes(x = bp_sd, y = AIC)) +
  geom_point(aes(colour = model)) +
  facet_wrap(~model) +
  theme_bw() +
  scale_color_manual(values = c("grey10", "grey30", "grey50", "darkviolet")) +
  theme(panel.grid.minor = element_blank())
  
```
So there is no pattern here at first sight. What we will want to do in this case is to re-do the simulation to confirm that the AIC shape remains stable. If so, we can take another pass looking at whether we can define a good metric for cal curve variability (if we are thus inclined).



Fourth - and that's the interesting part - the AIC/BIC behavior differs between the models for the 68% HPD areas and 95% HPD areas. For the former, while variable, we see no overall trend and the sigma_only model has higher AIC/BIC. The 95% HPD models have a clear downward trend and a much smaller gap between the AIC/BICs for different model types. The small gap implies is that, for the 95% models, measurement error alone is almost as good as knowing the offset magnitude at calling whether the result is off-target or not. This is something that will require some more digging into. Second, all models become better as we go back in time for the 95%. As the main difference is in the precision of the calibration curve, this implies that curve ucnertainty may play a role in how well a logistic regression will predict model fit. 


With these ideas now in mind, lets have a look at how different points in time compare. At first, we shall use the interaction model to investigate (it has best AIC and also makes most sense logically - but lets see the outcomes first).

```{r}
single_cals_log_results <- read_csv("model_results/single_cals_log_results.csv")
```



```{r}
single_cals_log_results %>%
  filter(str_detect(model, "interact") & !str_detect(term, "Intercept")) %>%
  filter(!(target_year %in% c(7019, 7751, 9076, 11091) & 
             str_detect(model, "95"))) %>%  ## This removes extreme values of the 95% model in the visualizations
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset"),
    model = if_else(str_detect(model, "68"), "68.2% HPD", "95.4% HPD")
  ) %>%
  ggplot(aes(x = target_year, y = estimate)) +
  geom_ribbon(aes(ymin = estimate - std.error * 2, 
                  ymax = estimate + std.error * 2,
                  fill = term), alpha = 0.2, color = "grey75") +
  geom_line(aes(color = term), linetype = "dotted") +
  facet_grid(cols = vars(offset_pos), rows = vars(model)) +
  ylim(c(-2, 2)) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  scale_fill_manual(name = "Parameter", values = c("steelblue", "grey50", "darkblue")) +
  scale_color_discrete(name = "Parameter")
```

Some quick notes.

Big blobby bits in the 95% model. Lets check what they are.

```{r}
single_cals_log_results %>%
  filter(std.error >0.75 & !str_detect(term, "tercept"))
```

There are a few observations at hand here. 

_First_, there were some extreme values for the models covering the 95% HPD range. These have been removed from the figure, but likely correspond to the models that did not want to converge (see warnings up top!).

_Second_, we can see that the 95% models have much greater uncertainties than the 68% models - in other words they are not as good precise as the estimates from the 68% HPD ranges. This does make sense - the 95% ranges will have a much greater number of on-terget estimates than the 68% ranges - therefore being more difficult to treat with logistic regression. 

_Third_, we see some variation in regression parameters through time. To decide whether it is meaningful in the context of the current sample, we need to check if the estimates from year to year are different enough from one another. There are two ways we could do it. One is to run Chi-squared tests to compare the two distributions. This would be more mathematically cohesive. Second way is to see if the parameter means are more than two standard deviations from one another - which gives us an estimate of how big the differences are. It would be prudent to do both, so this is what we will focus on here. 

```{r}
## this will get a little messy. In order for the thing to work efficiently, it will be easier to pivot the table wider
single_cals_log_results_wider_pos <- single_cals_log_results %>%
  filter(str_detect(model, "interact") & offset_pos) %>%
  select(!c(statistic, p.value)) %>%
  pivot_wider(names_from = term, values_from = c(estimate, std.error))

single_cals_log_results_wider_neg <- single_cals_log_results %>%
  filter(str_detect(model, "interact") & !offset_pos) %>%
  select(!c(statistic, p.value)) %>%
  pivot_wider(names_from = term, values_from = c(estimate, std.error))
```

```{r}
## Now lets figure out the for loops to create what we're after. While there likely is a smarter way of doing this, given the size of the data, for loop will work just fine. There will be a whole bunch of vectors that we will end up filling up with the differences between rows and then putting into a single DF for visualization.

# neg_offset_dif <- c()
# neg_offset_chi <- c()
# neg_intercept_dif <- c()
# neg_intercept_chi <- c()
# neg_error_dif <- c()
# neg_error_chi <- c()
# neg_interact_dif <- c()
# neg_interact_chi <- c()
# 
# pos_offset_dif <- c()
# pos_offset_chi <- c()
# pos_intercept_dif <- c()
# pos_intercept_chi <- c()
# pos_error_dif <- c()
# pos_error_chi <- c()
# pos_interact_dif <- c()
# pos_interact_chi <- c()
# 
# for (i in 2:100) {
#   ## Yes we're doing this with a for loop, using the knowledge that the tables have 100 observations each.
#   neg_offset_mu_curr <- single_cals_log_results_wider_neg$estimate_offset_magnitude[i]
#   neg_offset_mu_prev <- single_cals_log_results_wider_neg$estimate_offset_magnitude[i-1]
#   
#   neg_offset_sd_curr <- single_cals_log_results_wider_neg$std.error_offset_magnitude[i]
#   neg_offset_sd_prev <- single_cals_log_results_wider_neg$std.error_offset_magnitude[i-1]
#   
#   neg_offset_norm_dif <- (neg_offset_mu_curr - neg_offset_mu_prev) /
#                          sqrt((neg_offset_sd_curr^2 + neg_offset_sd_prev^2) / 2)
#   neg_offset_dif <- c(neg_offset_dif, neg_offset_norm_dif)
#   
# }
# 
# neg_offset_dif
## Kill this, too ugly to work
```

```{r}
#### I'm leaving the above for posterity to see the error of my ways.... this approach is too ugly. What we really need is two functions that will produce the vectors required. An estimate difference calculator and a chi squared estimator. What they'll need to do is to take on a vector of estimates, a vector of standard deviations and return a vector of the desired type of results.



estimate_running_diff <- function(estimates, stdevs) {
  ###Note assumptions: vectors longer than two, and equal length!
  ###If ever planned to publish the function would need a check and a warning for input lengths!
  
  difs <- c() ## our results will go here
  
  for (i in 2 : length(estimates)) {
    estimate_curr <- estimates[i] # A flourish - makes it easier for the likes of me to read!
    estimate_prev <- estimates[i-1]
    stdev_curr <- stdevs[i]
    stdev_prev <- stdevs[i-1]
    
    estimate_diff <- (estimate_curr - estimate_prev) /
                     (sqrt((stdev_curr^2 + stdev_prev^2)/2))
    
    difs <- c(difs, estimate_diff)
  }
  
  difs
  
}

estimate_running_diff(single_cals_log_results_wider_neg$estimate_offset_magnitude,
                      single_cals_log_results_wider_neg$std.error_offset_magnitude)


```




_Fourth_, we see that the interaction parameter is very close to zero. This implies we could get similar results by using a simpler model. I find this very counter-intuitive - I suspect that the term for impacts of measurement precision (measurement_error) might start turning the other way, i.e. right now the measurement error estimates tend to imply that increasing the measurement error deteriorates the accuracy of the calibrated date models. Lets see if this will remain the case under different model specifications. Lets also check how this will affect the offset parameter under the different models. As the effects are greater under the 68% HPD models, this is where I'll focus attention.

```{r}
single_cals_log_results %>%
  filter(!str_detect(model, "95") & 
           !str_detect(model, "sigma_only") & 
           !str_detect(term, "Intercept") & 
           !str_detect(term, "error:offset")) %>%  ## This removes extreme values of the 95% model in the visualizations
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
    )%>%
  ggplot(aes(x = target_year, y = estimate)) +
  geom_hline(yintercept = 0, size = 0.5) +
  geom_ribbon(aes(ymin = estimate - std.error * 2, 
                  ymax = estimate + std.error * 2,
                  fill = model), alpha = 0.2, color = "grey75") +
  geom_line(aes(color = model)) +
  facet_grid(cols = vars(offset_pos), rows = vars(term)) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  scale_fill_manual(name = "Model", values = c("steelblue", "grey50", "darkblue")) +
  scale_color_discrete(name = "Model")
```

OK, suspicion confirmed. What we see here is that the models without the interaction terms are on the one hand much more precise as far as their estimates go, but on the other hand, the parameter for the measurement error does indeed flip sigm - in the interaction models its mean is negative (implies greater scatter of estimates leads to greater chance of measurement being off target), while in the non-interaction models it is positive - implying that greater measurement uncertainty leads to greater precision. Now, we know that in the case of zero offset from the mean of the calibration curve, a super-precise determination would almost always fit within the uncertainty of the calibration curve - and therefore return more accurate results. As such, due to underpinning maths, the interaction model is a better description of reality even if the interaction term itself is centered around zero and the overall parameter estimate uncertainties are far higher.
