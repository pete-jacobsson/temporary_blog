---
title: "14C Power/Sensitivity notes"
author: "Piotr Jacobsson"
format: html
---

```{r}
library(tidyverse)
library(broom)
```


This document contains the research notes from the analysis of the simulations conducted using R and OxCal. This is a rough notebook that begins at the point when the project was re-initiated in June 2023 until completion

##Part 1: make sense of what is here and order things up
The first thing that I spotted was that much of the data files were sitting right in the main folder. So this will get ordered up - but I'll leave the Markdown files describing everything on top (I might need them lets see if past me had any brains worth of brain :P).

Next will be working out what the individual simulations are and what they cover - as well as taking some time to play with the Shiny App that I built to support the EDA. 
The thing looks much better now.

Lets have a look at the list of simulations we have.

```{r}
list_of_sims <- read_csv("sims_list.csv")
list_of_sims
```
There are a few things I could do here. I could start with the single calibrations and take it from there. Not sure what this would contribute though and I'd likely get bored. Also, there is the question of whether I want to keep working with test groups... Still could be fun refreshing what matters and chacking whether the calibration curve has anything to say about accuracy here.

For the wiggle-match dates, there are a few things that could be done. The first is to run models for the steep section and for the plateau and compare results. The interesting thing to do here would maybe be to use Stan while I am at it? The data sets are not that large and even if they turn out to be, I can truncate them. Yes, this could work.
Of course then there is the 300k project. It covers 1500 years, so the simulation density is 200 sims per year. Here I remember I wanted to go decade by decade, possibly with different regression models and start comparing. So we have options here for accuracy.
I might leave Sequences out at first pass.

Likewise I think it might be best to focus on accuracy early on - precision can come second. 

Oh and the Shiny will need a batch file. 

Had a quick look at Shiny. Next instalment should likely be some EDA with what I've got in the app and a decision if I want to maybe begin by completing that app.



# Single calibrations

## Part 2: Exploring the single radiocarbon dates with Shiny, and some prelim models

After some reflection I decided to start off with doing some model-based exploration of single calibrations. Using the Shiny App I was able to make some observations that helped.

1. The nominal accuracy was more or less OK for offsets up to about 15 - 25 14C years.
2. Greater measurement errors seem to mean more accurate dates
3. Measurements from different parts of the calibration curve may have different susceptibility to offsets. This seems to work different for positive and negative offsets.
4. Depending on the location on the calibration curve it may be that different offset directions have different effects.

Knowing this, the next step is to explore if any of these initial observations make it through exploratory modelling. The goal is to check is different parts of the calibration curve are comparable when it comes to effects of offset magnitudes and their interactions with measurement error magnitude.




```{r}
### Load the data
single_cals <- read_csv("simulation_results/singles_011_results.csv")
```


```{r}
###Seperate pos from neg offsets and group things by cal curve 
single_cals_modelled <- single_cals %>%
  mutate (
    offset_pos = if_else(offset_magnitude > 0, TRUE, FALSE),
    offset_magnitude = if_else(offset_pos, 
                               offset_magnitude, offset_magnitude * -1),
    # Changing negs on offset magnitude to have a consistent direction at downstream visualization.
    binned_targets = ntile(target_year, 50)
  ) %>%
  select(-target_year)


single_cals_modelled <- single_cals %>%
  mutate(binned_targets = ntile(target_year, 50)) %>%
  group_by(binned_targets) %>%
  summarize( #Easy way to get years for the indiv bins, without too muchj headach
    target_year = min(target_year)
    ) %>%
  inner_join(single_cals_modelled) %>% #Join the binned DF back in. No, I don't like this either!
  group_by(offset_pos, target_year) %>%
  nest()


### Now lets specify a model to test if things work
# offset_only_acc68 <- function(singles_data) {
#   glm(accuracy_68 ~ offset_magnitude, data = singles_data, family = binomial(link = 'logit'))
# }

### After which we apply the model across the DF and check if everythign works as it should.


# single_cals_modelled <- single_cals_modelled %>%
#   mutate(offset_only_acc68 = map(data, offset_only_acc68))
# 
# summary(single_cals_modelled$offset_only_acc68[[1]])
##Comment out after testing to avoid clashes

## Great! Now build remaining models and apply to the DF. 

```

## Part 3 Fitting multiple logistic regressions
In this step I want to fit multiple logistic regressions to single calibrations I have the data frame ready, next step is to write the regression functions and apply them to the DF. 

```{r}
### Build functions to use with map
## Could have tried in-line but it would be very confusing with the number of parameters floating about. 

offset_only_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ offset_magnitude, data = singles_data, 
      family = binomial)
}

sigma_only_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_interact_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + 
        measurement_error * offset_magnitude, 
      data = singles_data, family = binomial(link = 'logit'))
}


offset_only_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ offset_magnitude, data = singles_data, 
      family = binomial(link = 'logit'))
}

sigma_only_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_interact_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + 
        measurement_error * offset_magnitude, 
      data = singles_data, family = binomial(link = 'logit'))
}


```

```{r}
### Now use map to apply the models across the data
single_cals_modelled <- single_cals_modelled %>%
  mutate(
    offset_only_acc68 = map(data, offset_only_acc68),
    sigma_only_acc68 = map(data, sigma_only_acc68),
    offset_sigma_acc68 = map(data, offset_sigma_acc68),
    offset_sigma_interact_acc68 = map(data, offset_sigma_interact_acc68),
    offset_only_acc95 = map(data, offset_only_acc95),
    sigma_only_acc95 = map(data, sigma_only_acc95),
    offset_sigma_acc95 = map(data, offset_sigma_acc95),
    offset_sigma_interact_acc95 = map(data, offset_sigma_interact_acc95)
  )

```


## Part 4: Unpack the ensted lists and re-organize the DFs

Right now I have one DF with a lot of nested models. These need unpacked and re-organized into a new DF if they are to be visualizable (which is our objective after all!).

Lets try to loop through the unpacking and then create two DFs - one for parameters and one for model quality indicators. That would also be a good point to save the DFs into csv files (so we don't need to keep re-building them each time we re-visit the data).

```{r}
model_names <- colnames(single_cals_modelled)[4:11]

single_cals_log_results <- data.frame()
single_cals_log_diagnostics <- data.frame()

for (model in model_names) {
  ## This will get a little experimental - we are trying to create a big old table!
  ## First, we thin down the DF to the model of interest
  temp_results <- single_cals_modelled %>%
    select(1, 2, all_of(model)) %>% #All of used to address deprecation
    rename(glm_list = 3) %>% #This rename allows map to work correctly
    mutate(
      logistic_results = map(glm_list, tidy), # Do the map and also take note of which model
      model = model
    ) %>%
    select(-glm_list) %>% # Gets rid of the actual models (necessary for saving as csv)
    unnest(logistic_results)# Unnests results
  
  temp_diagnostics <- single_cals_modelled %>%
    select(1, 2, all_of(model)) %>%
    rename(glm_list = 3) %>%
    mutate(
      logistic_diagnostics = map(glm_list, glance),
      model = model
    ) %>%
    select(-glm_list) %>%
    unnest(logistic_diagnostics)
  
  single_cals_log_results <- rbind(single_cals_log_results, temp_results)
  single_cals_log_diagnostics <- rbind(single_cals_log_diagnostics, 
                                       temp_diagnostics)

}


write_csv(single_cals_log_results, 
          "model_results/single_cals_log_results.csv")
write_csv(single_cals_log_diagnostics, 
          "model_results/single_cals_log_diagnostics.csv")

```



## Part 5: Start plotting what all the results mean :D

There are two things we are interested in here. First is, how do different parameters for different models vary with time. Second, what are the model quality indicators. I'll start by looking at the latter, more specifically at AIC and BIC values.

```{r}
single_cals_log_diagnostics <- read_csv("model_results/single_cals_log_diagnostics.csv")

```

```{r}

## Lets make some plots
single_cals_log_diagnostics %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "68")) %>%
  ggplot(aes(x = target_year, y = AIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos) +
  theme_bw() +
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  labs(x = "Cal yrs BP")


single_cals_log_diagnostics %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "68")) %>%
  ggplot(aes(x = target_year, y = BIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos, labeller = labeller(c("Pos offset", "Neg offset"))) +
  theme_bw() +
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    )) +
  labs(x = "Cal yrs BP")

single_cals_log_diagnostics %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "95")) %>%
  ggplot(aes(x = target_year, y = AIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos) +
  theme_bw()+
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    )) +
  labs(x = "Cal yrs BP")


single_cals_log_diagnostics %>%
   mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "95")) %>%
  ggplot(aes(x = target_year, y = BIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos) +
  theme_bw() +
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    )) +
  labs(x = "Cal yrs BP")


  
```

There are a few things to note here. 
First, as expected the Sigma-only model performs worst of the whole group.

Second, the remaining models are close to each other in performance, with the interaction model performing marginally better with respect to AIC and the offset only model performaing little better with respect to BIC

Third, the model AIC and BIC varies with the set of target dates - suggesting that the performance of logistic regression varies depending on the shape of the calibration curve at a given segment of time. Lets explore if we can find a pattern by adding in calibration curve information and drawing some scatter plots.

```{r}
## Can't just bin cal curve. Does not match well
## First get cal curve and min/max for the actual bins

cal_curve <- read_csv("intcal_20_interpolated.csv")

binned_sim_yrs <- single_cals %>%
  mutate(bin_yrs = ntile(target_year, 50)) %>%
  group_by(bin_yrs) %>%
  summarize(
    bin_end = max(target_year),
    target_year = min(target_year)
    
  )

## Now loop through each bin, and get BP sd for that group of years - a proxy for how variable the cal curve was (less = more plateau)
bp_sd <- c()

for (i in 1:nrow(binned_sim_yrs)) {
  binned_sd <- cal_curve %>%
    filter(CalBP >= binned_sim_yrs$target_year[i] & 
             CalBP <= binned_sim_yrs$bin_end[i]) %>%
    summarize(bin_sd = sd(BP)) %>%
    pull(bin_sd)
  
  bp_sd <- c(bp_sd, binned_sd)
}

binned_sim_yrs <- cbind(binned_sim_yrs, bp_sd) %>%
  select(target_year, bp_sd)

###Errors here. Re-work to ensure match!


inner_join(single_cals_log_diagnostics, binned_sim_yrs) %>%
  filter(str_detect(model, "68")) %>%
  ggplot(aes(x = bp_sd, y = AIC)) +
  geom_point(aes(colour = model)) +
  facet_wrap(~model) +
  theme_bw() +
  scale_color_manual(values = c("grey10", "grey30", "grey50", "darkviolet")) +
  theme(panel.grid.minor = element_blank())
  
```
So there is no pattern here at first sight. What we will want to do in this case is to re-do the simulation to confirm that the AIC shape remains stable. If so, we can take another pass looking at whether we can define a good metric for cal curve variability (if we are thus inclined).



Fourth - and that's the interesting part - the AIC/BIC behavior differs between the models for the 68% HPD areas and 95% HPD areas. For the former, while variable, we see no overall trend and the sigma_only model has higher AIC/BIC. The 95% HPD models have a clear downward trend and a much smaller gap between the AIC/BICs for different model types. The small gap implies is that, for the 95% models, measurement error alone is almost as good as knowing the offset magnitude at calling whether the result is off-target or not. This is something that will require some more digging into. Second, all models become better as we go back in time for the 95%. As the main difference is in the precision of the calibration curve, this implies that curve ucnertainty may play a role in how well a logistic regression will predict model fit. 


With these ideas now in mind, lets have a look at how different points in time compare. At first, we shall use the interaction model to investigate (it has best AIC and also makes most sense logically - but lets see the outcomes first).

```{r}
single_cals_log_results <- read_csv("model_results/single_cals_log_results.csv")
```

First lets plot the intercept

```{r}
single_cals_log_results %>%
  filter(str_detect(model, "interact") & str_detect(term, "Intercept")) %>%
  filter(!(target_year %in% c(7019, 7751, 9076, 11091) & 
             str_detect(model, "95"))) %>%  ## This removes extreme values of the 95% model in the visualizations
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset"),
    model = if_else(str_detect(model, "68"), "68.2% HPD", "95.4% HPD")
  ) %>%
  ggplot(aes(x = target_year, y = estimate)) +
  geom_hline(yintercept = 0) +
  geom_ribbon(aes(ymin = estimate - std.error * 2, 
                  ymax = estimate + std.error * 2,
                  fill = offset_pos), alpha = 0.2, color = "grey75") +
  geom_line(aes(color = offset_pos), linetype = "dotted") +
  facet_grid(cols = vars(offset_pos), rows = vars(model), scales = "free") +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    )
```




Now lets plot the remaining parameters.
```{r}
single_cals_log_results %>%
  filter(str_detect(model, "interact") & !str_detect(term, "Intercept")) %>%
  filter(!(target_year %in% c(7019, 7751, 9076, 11091) & 
             str_detect(model, "95"))) %>%  ## This removes extreme values of the 95% model in the visualizations
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset"),
    model = if_else(str_detect(model, "68"), "68.2% HPD", "95.4% HPD")
  ) %>%
  ggplot(aes(x = target_year, y = estimate)) +
  geom_ribbon(aes(ymin = estimate - std.error * 2, 
                  ymax = estimate + std.error * 2,
                  fill = term), alpha = 0.2, color = "grey75") +
  geom_line(aes(color = term), linetype = "dotted") +
  facet_grid(cols = vars(offset_pos), rows = vars(model)) +
  ylim(c(-2, 2)) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  scale_fill_manual(name = "Parameter", values = c("steelblue", "grey50", "darkblue")) +
  scale_color_discrete(name = "Parameter")
```

Some quick notes.

Big blobby bits in the 95% model. Lets check what they are.

```{r}
single_cals_log_results %>%
  filter(std.error >0.75 & !str_detect(term, "tercept"))
```

There are a few observations at hand here. 

_First_, there were some extreme values for the models covering the 95% HPD range. These have been removed from the figure, but likely correspond to the models that did not want to converge (see warnings up top!).

_Second_, we can see that the 95% models have much greater uncertainties than the 68% models - in other words they are not as good precise as the estimates from the 68% HPD ranges. This does make sense - the 95% ranges will have a much greater number of on-terget estimates than the 68% ranges - therefore being more difficult to treat with logistic regression. 

_Third_, we see some variation in regression parameters through time. To decide whether it is meaningful in the context of the current sample, we need to check if the estimates from year to year are different enough from one another. We'll build a Z-test.
```{r}
## this will get a little messy. In order for the thing to work efficiently, it will be easier to pivot the table wider
single_cals_log_results_wider_pos <- single_cals_log_results %>%
  filter(str_detect(model, "interact") & offset_pos) %>%
  select(!c(statistic, p.value)) %>%
  pivot_wider(names_from = term, values_from = c(estimate, std.error))

single_cals_log_results_wider_neg <- single_cals_log_results %>%
  filter(str_detect(model, "interact") & !offset_pos) %>%
  select(!c(statistic, p.value)) %>%
  pivot_wider(names_from = term, values_from = c(estimate, std.error))
```



```{r}
#### I'm leaving the above for posterity to see the error of my ways.... this approach is too ugly. What we really need is two functions that will produce the vectors required. An estimate difference calculator and a chi squared estimator. What they'll need to do is to take on a vector of estimates, a vector of standard deviations and return a vector of the desired type of results.



estimate_running_diff <- function(estimates, stdevs) {
  ###Note assumptions: vectors longer than two, and equal length!
  ###If ever planned to publish the function would need a check and a warning for input lengths!
  
  difs <- c() ## our results will go here
  
  for (i in 2 : length(estimates)) {
    estimate_curr <- estimates[i] # A flourish - makes it easier for the likes of me to read!
    estimate_prev <- estimates[i-1]
    stdev_curr <- stdevs[i]
    stdev_prev <- stdevs[i-1]
    
    estimate_diff <- (estimate_curr - estimate_prev) /
                     (sqrt(stdev_curr^2 + stdev_prev^2))
    
    difs <- c(difs, estimate_diff)
  }
  
  difs
  
}

estimate_running_diff(single_cals_log_results_wider_neg$estimate_offset_magnitude,
                      single_cals_log_results_wider_neg$std.error_offset_magnitude)

## Now build the df. Yes, there is a smarter way...
z_test_comparisons <- data.frame(
  
  target_year = single_cals_log_results_wider_pos$target_year[2:100],
  pos_offset = estimate_running_diff(
    single_cals_log_results_wider_pos$estimate_offset_magnitude,
    single_cals_log_results_wider_pos$std.error_offset_magnitude),
  pos_error = estimate_running_diff(
    single_cals_log_results_wider_pos$estimate_measurement_error,
    single_cals_log_results_wider_pos$std.error_measurement_error
  ),
  pos_intercept = estimate_running_diff(
    single_cals_log_results_wider_pos$`estimate_(Intercept)`,
    single_cals_log_results_wider_pos$`std.error_(Intercept)`
  ),
  pos_interaction = estimate_running_diff(
    single_cals_log_results_wider_pos$`estimate_measurement_error:offset_magnitude`,
    single_cals_log_results_wider_pos$`std.error_measurement_error:offset_magnitude`
  ),
  neg_offset = estimate_running_diff(
    single_cals_log_results_wider_neg$estimate_offset_magnitude,
    single_cals_log_results_wider_neg$std.error_offset_magnitude),
  neg_error = estimate_running_diff(
    single_cals_log_results_wider_neg$estimate_measurement_error,
    single_cals_log_results_wider_neg$std.error_measurement_error
  ),
  neg_intercept = estimate_running_diff(
    single_cals_log_results_wider_neg$`estimate_(Intercept)`,
    single_cals_log_results_wider_neg$`std.error_(Intercept)`
  ),
  neg_interaction = estimate_running_diff(
    single_cals_log_results_wider_neg$`estimate_measurement_error:offset_magnitude`,
    single_cals_log_results_wider_neg$`std.error_measurement_error:offset_magnitude`
  )
  
) %>%
  pivot_longer(!target_year, names_to = "parameter", values_to = "z_value") %>%
  mutate(
    offset_direction = if_else(str_detect(parameter, "pos"), "pos", "neg"),
    parameter = str_remove(parameter, "pos_|neg_")
  )


```

```{r}
z_test_comparisons %>%
  ggplot(aes(x = target_year, y = z_value)) +
  geom_hline(yintercept = c(-2, 2), size = 0.5, linetype = "dashed") +
  geom_line(aes(color = offset_direction)) +
  facet_wrap(~parameter) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
   scale_color_manual( values = c("steelblue", "darkblue"))
```
On the whole, not much to talk about, a few instances of going over 2z, but this is not decisive given the number of data points we're talking about. Now, what is perhaps a little more interesting is that the Z values have greater swings between bins in most recent years, but more subtle ones the further back we go in time. 


_Fourth_, we see that the interaction parameter is very close to zero. This implies we could get similar results by using a simpler model. I find this very counter-intuitive - I suspect that the term for impacts of measurement precision (measurement_error) might start turning the other way, i.e. right now the measurement error estimates tend to imply that increasing the measurement error deteriorates the accuracy of the calibrated date models. Lets see if this will remain the case under different model specifications. Lets also check how this will affect the offset parameter under the different models. As the effects are greater under the 68% HPD models, this is where I'll focus attention.

```{r}
single_cals_log_results %>%
  filter(!str_detect(model, "95") & 
           !str_detect(model, "sigma_only") & 
           !str_detect(term, "Intercept") & 
           !str_detect(term, "error:offset")) %>%  ## This removes extreme values of the 95% model in the visualizations
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
    )%>%
  ggplot(aes(x = target_year, y = estimate)) +
  geom_hline(yintercept = 0, size = 0.5) +
  geom_ribbon(aes(ymin = estimate - std.error * 2, 
                  ymax = estimate + std.error * 2,
                  fill = model), alpha = 0.2, color = "grey75") +
  geom_line(aes(color = model)) +
  facet_grid(cols = vars(offset_pos), rows = vars(term)) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  scale_fill_manual(name = "Model", values = c("steelblue", "grey50", "darkblue")) +
  scale_color_discrete(name = "Model")
```

OK, suspicion confirmed. What we see here is that the models without the interaction terms are on the one hand much more precise as far as their estimates go, but on the other hand, the parameter for the measurement error does indeed flip sigm - in the interaction models its mean is negative (implies greater scatter of estimates leads to greater chance of measurement being off target), while in the non-interaction models it is positive - implying that greater measurement uncertainty leads to greater precision. Now, we know that in the case of zero offset from the mean of the calibration curve, a super-precise determination would almost always fit within the uncertainty of the calibration curve - and therefore return more accurate results. As such, due to underpinning maths, the interaction model is a better description of reality even if the interaction term itself is centered around zero and the overall parameter estimate uncertainties are far higher.



Now, what all this did not give us is reproduction of the trends we've seen in the EDA histograms. What I'd be keen to do is to try again with wider bins. There will be a big ole' copy paste comin' this way, I'm afraid. 


## Part 6 Lets check a different bin size
The outcomes above are deeply suspicious given a clear difference that came out in the EDA - there should be some difference in something between models for data in the more recent bits of the curve and those for more distant times. To this end, lets re-do the last few hundred lines, but at just ten bins (rather than 50).]

```{r}
### Load the data

single_cals <- read_csv("simulation_results/singles_011_results.csv")

###Seperate pos from neg offsets and group things by cal curve 
single_cals_binned10 <- single_cals %>%
  mutate (
    offset_pos = if_else(offset_magnitude > 0, TRUE, FALSE),
    offset_magnitude = if_else(offset_pos, 
                               offset_magnitude, offset_magnitude * -1),
    # Changing negs on offset magnitude to have a consistent direction at downstream visualization.
    binned_targets = ntile(target_year, 10)
  ) %>%
  select(-target_year)


single_cals_binned10 <- single_cals %>%
  mutate(binned_targets = ntile(target_year, 10)) %>%
  group_by(binned_targets) %>%
  summarize( #Easy way to get years for the indiv bins, without too muchj headach
    target_year = min(target_year)
    ) %>%
  inner_join(single_cals_binned10) %>% #Join the binned DF back in. No, I don't like this either!
  group_by(offset_pos, target_year) %>%
  nest()

```

```{r}
### Now use map to apply the models across the data structured into only 10 bins
single_cals_modelled_bin10 <- single_cals_binned10 %>%
  mutate(
    offset_only_acc68 = map(data, offset_only_acc68),
    sigma_only_acc68 = map(data, sigma_only_acc68),
    offset_sigma_acc68 = map(data, offset_sigma_acc68),
    offset_sigma_interact_acc68 = map(data, offset_sigma_interact_acc68),
    offset_only_acc95 = map(data, offset_only_acc95),
    sigma_only_acc95 = map(data, sigma_only_acc95),
    offset_sigma_acc95 = map(data, offset_sigma_acc95),
    offset_sigma_interact_acc95 = map(data, offset_sigma_interact_acc95)
  )

```

```{r}
model_names <- colnames(single_cals_modelled_bin10)[4:11]

single_cals_log_results_bin10 <- data.frame()
single_cals_log_diagnostics_bin10 <- data.frame()

for (model in model_names) {
  ## This will get a little experimental - we are trying to create a big old table!
  ## First, we thin down the DF to the model of interest
  temp_results <- single_cals_modelled_bin10 %>%
    select(1, 2, all_of(model)) %>% #All of used to address deprecation
    rename(glm_list = 3) %>% #This rename allows map to work correctly
    mutate(
      logistic_results = map(glm_list, tidy), # Do the map and also take note of which model
      model = model
    ) %>%
    select(-glm_list) %>% # Gets rid of the actual models (necessary for saving as csv)
    unnest(logistic_results)# Unnests results
  
  temp_diagnostics <- single_cals_modelled_bin10 %>%
    select(1, 2, all_of(model)) %>%
    rename(glm_list = 3) %>%
    mutate(
      logistic_diagnostics = map(glm_list, glance),
      model = model
    ) %>%
    select(-glm_list) %>%
    unnest(logistic_diagnostics)
  
  single_cals_log_results_bin10 <- rbind(single_cals_log_results_bin10, temp_results)
  single_cals_log_diagnostics_bin10 <- rbind(single_cals_log_diagnostics_bin10, 
                                         temp_diagnostics)

}


write_csv(single_cals_log_results_bin10, 
          "model_results/single_cals_log_results_bin10.csv")
write_csv(single_cals_log_diagnostics_bin10, 
          "model_results/single_cals_log_diagnostics_bin10.csv")

```


Now lets re-load the results and re-plot the parameters to see if anything changes. 
```{r}
single_cals_log_results_bin10 <- read_csv(
  "model_results/single_cals_log_results_bin10.csv")
```

First lets plot the intercept

```{r}
single_cals_log_results_bin10 %>%
  filter(str_detect(model, "interact") & str_detect(term, "Intercept")) %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset"),
    model = if_else(str_detect(model, "68"), "68.2% HPD", "95.4% HPD")
  ) %>%
  ggplot(aes(x = target_year, y = estimate)) +
  geom_ribbon(aes(ymin = estimate - std.error * 2, 
                  ymax = estimate + std.error * 2,
                  fill = model), alpha = 0.2, color = "grey75") +
  geom_line(aes(color = model), linetype = "dotted") +
  facet_grid(cols = vars(offset_pos), rows = vars(model), scales = "free") +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(colour = "grey80", size = 0.1),
    panel.border = element_blank(),
    axis.line = element_line(colour = "grey50", size = 0.5),
    legend.position = "none",
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  scale_fill_manual(values = c("steelblue", "darkblue")) +
  scale_color_manual(values = c("steelblue", "darkblue"))
```

So what we can see here is that, while the estimates are still effectively similar, the intercept does seem to go up the further back we go in time. 

```{r}
single_cals_log_results_bin10 %>%
  filter(str_detect(model, "interact") & !str_detect(term, "Intercept")) %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset"),
    model = if_else(str_detect(model, "68"), "68.2% HPD", "95.4% HPD")
  ) %>%
  ggplot(aes(x = target_year, y = estimate)) +
  geom_ribbon(aes(ymin = estimate - std.error * 2, 
                  ymax = estimate + std.error * 2,
                  fill = term), alpha = 0.2, color = "grey75") +
  geom_line(aes(color = term), linetype = "dotted") +
  facet_grid(cols = vars(offset_pos), rows = vars(model)) +
  #ylim(c(-0.25, 0.25)) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(colour = "grey80", size = 0.1),
    panel.border = element_blank(),
    axis.line = element_line(colour = "grey50", size = 0.5),
    #legend.position = "none",
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  scale_fill_manual(name = "Parameter", values = c("steelblue", "grey50", "darkblue")) +
  scale_color_discrete(name = "Parameter")
```
Everything here is kinda flat. So the difference seems to live in the intercept - which makes sense. At this point variation due calibration curve can be ignored (we just used bins that would aggregate a lot of the calibration curve). This implies that the differences that we can detect with this set of simulations are not due to the fine shape of the curve, but maybe rather due to the magnitude of the unceartainty about the calibration curve estimate itself. 


## Part 7 Exploring models that include calibration curve uncertainty

To check whether this is the case, lets take a different approach and see whether uncertainty about the calibration curve may have some kind of a role to play. Now, there exists a risk that this might not be a good estimate at points such as calibration plateaux or steep sections, so we'll need to do two bits of prep:

1. Join the data to calibration curve uncertainty at given target date
2. Set chunks of the simulated data aside.

```{r}
single_cals_w_curve_uncert <- read_csv("intcal_20_interpolated.csv") %>%
  select(CalBP, Error) %>%
  rename(target_year = CalBP, curve_uncert = Error) %>%
  inner_join(single_cals)
```
```{r}
## Take a quick look at how curve uncertainty goes with accuracy

single_cals_w_curve_uncert %>%
  mutate(curve_uncert = round(curve_uncert)) %>%
  group_by(curve_uncert) %>%
  summarize(
    `68.2% HPD` = mean(accuracy_68),
    `95.4% HPD` = mean(accuracy_95)
  ) %>%
  pivot_longer(!curve_uncert, values_to = "accuracy", names_to = "hpd_area") %>%
  ggplot(aes(x = curve_uncert, y = accuracy)) +
  geom_bar(stat = 'identity', fill = "steelblue") +
  facet_wrap(~hpd_area)+
  labs(
    x = "Calibration curve uncertainty",
    y = "Ratio accurate"
  ) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    axis.line = element_line(colour = "grey50", size = 0.5),
    #legend.position = "none",
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel")
    )
  
```

OK, we can see that there is a relationship between calibration curve uncertainty and accuracy of the different modelled HPD areas.

Next step is to build a few models and compare their performance. Lets recall that we want to keep some of the data out, to ensure that we can use it to test the models at the end of the process. 
To do this, I chose the following periods:
- All data from 2400 to 2700 cal BP (a calibration plateau)
- All data from 6500 to 6800 cal BP (a "straight" section of the curve)
- All data from 9400 to 9600 cal BP

```{r}
###Step 1. Extract bits that we want to use for model validation
single_cals_curve_uncert_val <- single_cals_w_curve_uncert %>%
  filter((target_year >= 2400 & target_year <= 2700) | 
         (target_year >= 6500 & target_year <= 6800) | 
         (target_year >= 9400 & target_year <= 9500))

single_cals_curve_uncert_mod <- single_cals_w_curve_uncert %>%
  filter(!((target_year >= 2400 & target_year <= 2700) | 
           (target_year >= 6500 & target_year <= 6800) | 
           (target_year >= 9400 & target_year <= 9500)))

## Now these get put into csv files for use later:

write_csv(single_cals_curve_uncert_val, "single_cals_curve_uncert_val.csv")
write_csv(single_cals_curve_uncert_mod, "single_cals_curve_uncert_mod.csv")
```

```{r}
### Now load up the model data again, nest it and build the model functions
single_cals_curve_uncert_mod <- read_csv("single_cals_curve_uncert_mod.csv")
```

```{r}
##Nesting
single_cals_curve_uncert_nest <- single_cals_curve_uncert_mod %>%
  mutate(is_pos = if_else(offset_magnitude >= 0, TRUE, FALSE)) %>%
  select(curve_uncert, measurement_error, offset_magnitude, accuracy_68,
         accuracy_95, is_pos) %>%
  group_by(is_pos) %>%
  nest()
```

```{r}
## Write model functions for 68% HPDs

offset_curve_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ offset_magnitude + curve_uncert, data = singles_data, 
      family = binomial)
}

sigma_curve_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + curve_uncert, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_curve_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_curve_interact_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ offset_magnitude + curve_uncert + 
        offset_magnitude * curve_uncert, 
      data = singles_data, 
      family = binomial)
}

sigma_curve_interact_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + curve_uncert +
        measurement_error * curve_uncert, 
      data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_sc_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + curve_uncert +
        measurement_error * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_oc_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_oc_sc_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * curve_uncert + measurement_error * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_oc_sc_ocs_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * curve_uncert + measurement_error * curve_uncert +
        offset_magnitude * measurement_error * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

```

```{r}
## And now have those set up for the 95% HPD ranges -- yes I could have pivoted longer. I know!

offset_curve_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ offset_magnitude + curve_uncert, data = singles_data, 
      family = binomial)
}

sigma_curve_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + curve_uncert, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_curve_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_curve_interact_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ offset_magnitude + curve_uncert + 
        offset_magnitude * curve_uncert, 
      data = singles_data, 
      family = binomial)
}

sigma_curve_interact_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + curve_uncert +
        measurement_error * curve_uncert, 
      data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_sc_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + curve_uncert +
        measurement_error * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_oc_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_oc_sc_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * curve_uncert + measurement_error * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_oc_sc_ocs_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * curve_uncert + measurement_error * curve_uncert +
        offset_magnitude * measurement_error * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

```


```{r}
### Now lets run the models...
single_cals_curve_uncert_regr <- single_cals_curve_uncert_nest %>%
  mutate(
    offset_curve_acc68 = map(data, offset_curve_acc68),
    offset_curve_acc95 = map(data, offset_curve_acc95),
    offset_curve_interact_acc68 = map(data, offset_curve_interact_acc68),
    offset_curve_interact_acc95 = map(data, offset_curve_interact_acc95),
    offset_sigma_curve_acc68 = map(data, offset_sigma_curve_acc68),
    offset_sigma_curve_acc95 = map(data, offset_sigma_curve_acc95),
    offset_sigma_curve_interact_oc_acc68 = 
      map(data, offset_sigma_curve_interact_oc_acc68),
    offset_sigma_curve_interact_oc_acc95 = 
      map(data, offset_sigma_curve_interact_oc_acc95),
    offset_sigma_curve_interact_oc_sc_acc68 = 
      map(data, offset_sigma_curve_interact_oc_sc_acc68),
    offset_sigma_curve_interact_oc_sc_acc95 = 
      map(data, offset_sigma_curve_interact_oc_sc_acc95),
    offset_sigma_curve_interact_oc_sc_ocs_acc68 = 
      map(data, offset_sigma_curve_interact_oc_sc_ocs_acc68),
    offset_sigma_curve_interact_oc_sc_ocs_acc95 = 
      map(data, offset_sigma_curve_interact_oc_sc_ocs_acc95)
  )

```

```{r}
### Lets re-orgnize the table to make parameter extraction easier.
single_cals_curve_uncert_regr <- single_cals_curve_uncert_regr %>%
  select(-data) %>%
  pivot_longer(!is_pos, names_to = "model", values_to = "model_results")

saveRDS(single_cals_curve_uncert_regr, "single_cals_curve_uncert_regr.rds")
```

```{r}
single_cals_curve_uncert_regr <- readRDS("single_cals_curve_uncert_regr.rds")
```


## Part 8: Compare the models with cal curve ucnertainty as a variable


