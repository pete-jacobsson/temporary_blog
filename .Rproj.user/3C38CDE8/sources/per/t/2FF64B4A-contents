---
title: "Not enough power for sensitivity: Compromises (weeks 6-10?)"
author: "Pete"
date: "2023-08-25"
categories: [14C Power/Sensitivity, Research Design, Project Log]
execute:
  message: false
format: 
  html: 
    code-fold: true
    code-overflow: scroll
---

```{r include=FALSE}
library(tidyverse)
library(broom)
```


Hello again! Well, this post was on the slow side in the coming... A few things happened. One of them was that I walked into a Microsoft PL300 run without having touched Power BI beforehand - lets say that it was (at least) a demanding route of advance and I promptly had to limit time on other projects. Of course this coincided with the time when a lot of time was what I needed to push this sensitivity project along. So what happened?


## Wandering estimates
When we left off, the aim was to shift the planned approach from one that tried to build different models for different points in time and then using those to predict if the radiocarbon measurements, given a certain offset magnitude, are going to be off-target or not. As an alternative, the plan was to include calibration curve uncertainty and check how it impacts model accuracy.

At first things were coming along nicely. I've set up a nice collection of models and had a look at the various model quality metrics.

```{r}
####HERE visualize the results
```

Encouraged by the results, I proceeded to check the model fit to the input data. It's here when things got slow going. As I was using a custom model, inputing the spec into the ggplot smooth function was not going to work. I needed to build a set of my own functions to do the task. It took a little bit of Stack Exchanging to come up with a plan and also the jtools package came in very handy - in particular it contained the function zzzz(), which can be used to build a set of model predictions, given an input model. 

```{r}
####HERE functions
```



Quite cheerful I proceeded to plot the results. They came back as pictured on the associated pictures: not matching the model very well. In particular the measurement error estimates seemed to be very far off what they'd need to be to match the data. 

```{r}
####HERE Illustrations
```


## If in doubt: combine!
At this stage I started suspecting that this has something to do with how curve uncertainty estimate affects the model. Long and short, it looked like the models built using both curve uncertainty data and the measurement error data might have been confounding the two effects. I decided to address this by a trick: combining the measurement error and the curve uncertainty into a single parameter. While not a pretty solution, in practice, for most samples, we would have some expectation of age prior to measurement and thus we could make a reasonable guess on the magnitude of curve uncertainty (indeed better than the guess as to what measurement precision the instruments would return). With this data in tow there would thus be the possibility of predicting model accuracy. 

This left me with two possible models to take to STAN: one with an interaction between the two parameters, one without such interaction. Both models had bearable AIC/BIC values compared to their counterparts. 

```{r}
####HERE Figure - compare AIC/BIC of curve_offset models!
```

The intercepts were also comparable within the models, though the model with interactions was giving meaningless predictions if we calculated the values at zero offset and uncertainty (basically plugging the intercept only into the logistic equation). Still, both models produced results that more or less fit the input data (with the stress on more or less...) and I've decided to take them both through to modelling with STAN.

```{r}
###HERE model results
```


Which we'll cover in the next installment... with three weeks of an intensive French course ahead I've got no idea when the next installment will be though!